\section {Background}

\chapter{Software Defined Networking} \label{chap:sdn} %% chapter 3

Computer networking is a vital part of the services that are offered today, and as such, the performance in technology backing these services is central to the quality of these services. As the service providers reorganize their
data centers in the cloud computing domain, enabling several improvements in the predictability, quality of service and ease of use of their services. New technologies are then required to make sure that their services are adapted
to the fast changing landscape of networking services. One of the most notable innovations in this field is called Software Defined Networking, because its architecture allows for two essential features

\begin {itemize}
    \item \textbf{Separation of network planes} SDN allows for the separation of the network control plane from the data forwarding plane by having network "intelligence" present in the network controllers, and having them
control the forwarding elements that live in the Data Plane
    \item \textbf{Centralization of network management functions} By isolating the management on a separate plane, there is possibility of developing a single controller that can regulate the entire network, having unrestricted access to every element present in the network, simplifying management, monitoring, application of QoS policies, flow optimization, ...
\end {itemize}

In this chapter we explore the essential characteristics of SDN, the technologies that provide the back end for the development of this technology, and current implementations of the most popular SDN controllers, so that we can 
see the features that should be present while developing a management interface for SDN controllers.

\section {Introduction}

As described in the previous section, there is a clear evolution of requirements, and this evolution was possible due to the adaptation of the exiting technologies to support better, and more efficient protocols that could carry 
the large amount of data that is transmitted every second. With that in mind, and in order to reduce costs to the service providers, simplify deployment and maintenance operations, developments in Software-Defined Networking (SDN) 
have been growing since 2010.

\par This new paradigm introduces programmability in the configuration and management of networks, by consolidating the control of network devices to a single central controller, achieving separation of the control and the 
data plane, and supporting a more dynamic and flexible infrastructure. Another important paradigm, that follows the development of SDN, is the concept of Network Function Virtualization. This concept allows to remove the amount of 
\textit {middleboxes}\footnote {Computer networking device that does some operations on traffic, excepting packet forwarding. Examples include caches, IDS's, NAT's, ..}, by replacing these with generic software 
applications.

\par The essence of SDN is described in a short manner by the Open Networking Foundation (ONF) paper: \textit{ In the SDN architecture, the control and data planes are decoupled,  network intelligence and state are 
logically centralized, and the underlying network infrastructure is abstracted from the applications}. The following picture defines both of the approaches to network, the traditional one and the SDN way, that considers 
that application and control traffic should be handled separately.

\begin{figure}[!tbph]
  \centering
  \subfloat[Traditional networking architecture]{\includegraphics[width=0.4\textwidth]{bib/network_trad}\label{fig:net_trad}}
  \hfill
  \subfloat[SDN architecture]{\includegraphics[width=.4\textwidth]{bib/network_sdn}\label{fig:net_sdn}}
  \caption {Traditional vs SDN network architecture}
\end{figure}

\par One example of the operation of a switch in the SDN model is the following:

\begin {itemize} 
    \item A switch runs an agent, and this agent is connected to a controller;
    \item This controller runs software that can operate the network, managing flow control rules, and collecting information, enabling a full view of the network from a central node
    \item While this controller can be logically centralized, for fault-tolerance and high availability purposes it can be distributed, and by optimizing datamodels and providing caches, one of the earliest ONOS prototypes was
able to achieve a distributed Network OS that could be applied to production networks \cite {berde_onos:_2014}
\end {itemize}

\par After analysing some examples of deployment of the SDN model, the analysed literature provides some insights on the requirements that platforms using this paradigm need to obey \cite {masutani_requirements_2014}.
 
\begin {itemize} 
    \item \textbf{High Performance}   
    \item \textbf{High Availability} 
    \item \textbf{Fault Tolerance}   
    \item \textbf{Monitoring}   
    \item \textbf{Programmability}   
    \item \textbf{Security}   
\end {itemize}

\section {OpenStack}
\section {OpenFlow}

As the growth of the networking infrastructure of the past few decades became evident, the need for an enviroment that allows for experimentation and testing of different protocols and equipment became evident. If networking 
research would depend on the previously existing methods, then new ways of creating and developing protocols would become increasingly hard to implement and develop. As such, there was need for a framework that could 
enable testing of new ideas on close to realistic settings. So, on February 2011 the version 1.1 of OpenFlow was released, and this proposal quickly became the standard for networking in a Software Defined Network. Since
2011, this protocol has suffered some revisions, and the latest version supported is version 1.5.1. Since this framework has evolved quite a bit, this section focuses on the versions 1.3, which are the versions that 
are used in development of this dissertation. 
\par Several reasons led to the quick standardization of this protocol, which are related not only to the initial requirements of the platform, like the capability of supporting high-performance and low-cost implementations, 
and the capability of ensuring separation between production and testing traffic, but also the extensibility that the open source development model provides, removing the limitations that closed or commercial solutions give the 
network researchers.
\par The big advantage of OpenFlow is that it is, from the data forwarding plane point of view, easy to process. Since the control decisions are made by the controller, which lives in a separate plane, all the switch needs to do
is correctly match the incoming packets, and forward them according to the rules established by the controller. The components that are part of this system and enable this functionality are:

\begin {itemize}
    \item \textbf {FlowTables} This element describes the main component of the switching capabilities of the OpenFlow switch. Inside the switch there are several flow tables that can be used to match incoming packets,
and process them in the rules that are specified by the controller. These rules can contain actions that affect the path of the packets, and these actions usually include forwarding to a port, packet modification, among
others. Classification is done via matching one or more field present in the packet, for example the switch input port, the MAC and IP addresses, IP protocol, basically all information required to correctly process the 
incoming packet. The required actions for an OpenFlow switch are the capability of forwarding to a set of output ports, allowing the packet to move across the network; to send them to the controller, in the case of a
miss of match; and finally the ability to drop packets, which is useful for DDoS mitigation, or more security concerns.
    \item \textbf {OpenFlow Protocol} Through the establishment of the OpenFlow Protocol between the switch and the controller, there is the definintion of several messages that allow for the control of the switch. This protocol
enables capabilities such as adding, deleting and updating flow mods in the switch, that are refered to as \textit {Controller-to-Switch} messages. Other relevant message types are the \textit {Asynchronous}, that enable the
notification of some event that ocurred, this type includes the Packet-In message, that is a type of message that is sent to the controller when a certain packet has no match in the flow tables present in the switch; and the 
\textit{ Synchronous} message that enable functionality such as the Hello message, that is used to start the connection between the switch and the controller.
    \item \textbf {Secure Channel} OpenFlow defines the channel that is between the switch and the controller as a secure communications channel. As the messages that are sent to the switch are critical for the correct operation 
of the system, as indicated in the previous point, the channel should be criptographically secure, to prevent spoofing of this information. As such, the channel is tipically transported over TLS.
\end {itemize}

% XXX SOURCE This images were taken from : open flow switch specification  https://3vf60mmveq1g8vzn48q2o71a-wpengine.netdna-ssl.com/wp-content/uploads/2014/10/openflow-switch-v1.3.5.pdf
\begin{figure} [h]
    \begin{subfigure}
    \includegraphics[width=0.25\textwidth]{sdn/open_flow_switch_pipeline}
    \end{subfigure}
    \begin{subfigure}
    \includegraphics[width=0.6\textwidth]{sdn/open_flow_tables}
    \end{subfigure}
\caption{Images describing OpenFlow components. On the left, an overview to the entire system, and on the right a view at the table structure of the OpenFlow Switch}
\end{figure}

\section {SDN Controllers}
\subsection {Floodlight}
\subsection {OpenDaylight} \label{chap:odl}

\chapter{Network Management} \label{chap:nm} %% chapter 3

\par This chapter focuses on the management of networks, where we explore what is most necessary to obtain a comprehensive understanding of the network; formalize the statistics that can be reported via OpenFlow; see some 
research that has been done in the management of SDN applications, including what existing controllers provide us; and finally explore the way that DDoS detection and mitigation is usually implemented.

\section {Introduction}

\par As networks grow larger and more complex, systems must be put in place that allow for closely monitoring the resources that make up the network, while also allowing for a certain freedom for the possible constant change of the 
network. As such, typical vendor solutions don't really fit into this ever changing landscape, since they present very solid and vertically integrated solutions. The SDN paradigm, however, is able to solve this issue, since it 
enables for the centralized control of the underlying networks, which provides visibility and even control over the network, simplifying network diagnosis or troubleshooting. 
\par Although SDN is a promising paradigm in terms of networking management, it also introduces some points of failure that are non existing, or not as impactful in current networking deployments. This is related, for example,
to the centralization of the controller, which makes it susceptible to Denial-of-Service attacks or even the possibility of some malicious attacker that could possibly exploit the privileged view that the SDN controller has.
\par The topic of network management is very extensive, due to the several components that make up today's networks, and the vast amount of information that they provide. It can be summed up as the operation and maintenance 
of network infrastructure so that the service it provides is not only "healthy", but also is operated at a level that keeps costs down for service providers. 

\section {Requirements for management systems}

\par As the complexity of the networks, and network devices that compose them grow bigger and bigger, the management systems should accommodate for the their necessities. As such, the basic groups of requirements for management 
functions are that defined in the ITU-T X 700 Recommendation \cite {CITE - ITU T Recomendation} are:

\begin {itemize}
    \item \textbf {Fault management} is the capability for detection, isolation and correction of abnormal operation in the system
    \item \textbf {Accounting management} provides ways to monitor the system resource utilization, and using this data to generate information about the costs that the operation of a certain resource will incur. This 
        allows for better optimizing the network utilization of network, as it provides insights on how to plan the evolution of the network
    \item \textbf {Configuration management} is related to the maintenance and updates of hardware and software in the network, and the general setup of devices that allow to start, maintain and terminate services 
    \item \textbf {Performance management} relates to monitor systems for the traffic utilization, response time, performance and logging histories. This allows to maintain Service Level Agreements (SLA) from the service
        provider and the client, providing better services even in cases of unusual traffic.
    \item \textbf {Security management} enables setting up security policies in terms of access control to resources, private information protection, among others.
\end {itemize}


\par A network management system usually consists of a centralized station, and management agents running on the network devices. Using management protocols, the agents can report to the station information about the its operational 
status, which includes information ranging from CPU load to bandwith usage. Typically this information can be retrieved by the controller polling the agents, or the agents sending information on their own, usually to inform
status changes. Using this information, the network operator can get insight on the performance or possible errors of the devices that are monitored. In the next section, we explore one of the most popular management protocols,
SNMP.

\section {SNMP}

The Simple Network Management Protocol is an IETF defined protocol that allows for the interconnection of networking devices, and provide a structured way to retrieve relevant information about these devices. As the name suggests,
SNMP allows for a simplified approach to network monitoring, since it reduces the complexity of the functions that the management agent needs to comply with, which bring several advantages, like reducing
the costs for development of management tools; provides a way to monitor, independently from different hardware providers the resources; and also supports freedom in extending the protocol in order to include other aspects of 
network operation. \cite{CITE - RFC 1157} %% XXX cite https://tools.ietf.org/html/rfc1157
\par  The architectural model of SNMP can be described in figure \ref{fig:snmp}.
    
\begin{figure} [!htbp]
    \centering
    \includegraphics[width=.6\textwidth]{nm/snmp_arch}
    \caption{Architectural components of SNMP}
    \label{fig:snmp}
\end{figure}

The management database is one of the most important components of this system, because it serves as a reference to the entities that are managed in the SNMP protocol. The formal name for this database is the MIB - Management 
Information Base \cite {CITE - RFC 1155}, and its composed of a collection of objects.

\par Each object has a name, syntax and encoding \cite {CITE - RFC 1156}. The name of the object, more specifically, the \textit {Object Identifier (OID)}, is a reference to the object itself. This name is usually a 
list of integers, and they serve to build a tree-like hierarchy. This structure allows for the organization of all objects in a logical pattern, as there is a parent node, that contains references to their children, 
that provide different indexes for different objects. For human readability, there is usually a \textit {Object Descriptor}, to refer to the object type. The syntax defines the type of data structure in the object type; and 
the encoding describes how the object type is transmitted on the network. In the context of this thesis, an important group is the interfaces group, as this exposes information about the interfaces present in a system. It's OID 
is the .3.6.1.2.1.2., and contains the number of interfaces in a system, and a table containing the counters related to the interface status, like the received unicast packets, the physical address, among others. The flexibility of
the MIB allows for vendors to introduce their own databases into the MIB, while also remaining compatible with the standardized one.

\par Due to its permanence in the market, the protocol has suffered some large changes since its original design. SNMPv3 now supports important changes to the original one, most notably in the security aspects, introducing strong
authentication and encryption capabilities.

\section {Data Center Networks (DCN)}

\par The rising demand of services like music and video streaming, or mass data storage, brings an increase of demand of compute and storage infrastructures, and the shift to the cloud computing model has led to a
proliferation of large data-centers, containing thousands of physical nodes. Related to this growth is the focus on moving not only servers to a virtualised environment, by having one physical host several virtual machines and
client applications, but moving also the networking functions to a virtual environment, by replacing the dedicated network hardware with generic compute resources, in a paradigm called \textit{Network Function Virtualisation (NFV)} 
\cite {CITE - OPENSDWN}. One of the bigger gains of using NFV, is the possibility of separation of each virtual network (VN), which guarantees better performance isolation and application of Quality of Service (QoS) rules
\cite {CITE - Data center virtualisation a survey}. 
\par The design of the network architecture is central to the data-center networks, as the placement for physical hosts and virtual machines allows for sharing the resources and create a logical hierarchy of network devices. The 
study on the design of DCN has resulted in the creation of typical DC topologies, like fat-tree topologies (as seen in \ref {nm/fattree}), or others, including de Bruijn server only networks, or BCube switch heavy networks 
\cite{ popa_cost_2010 }. This approach allows for the traffic characteristics, resource consumption and costs of the networking devices be understood, so that causes for failure of this network are understood and mitigated, 
and the entire DC can run on the most optimal possible way. The organization in the DCN also allows for traffic in the network being resistant to failure scenarios, since there are multiple paths that can redirect packets 
to the correct destination, even if a link to a switch fails.

\begin{figure} [!htbp]
    \centering
    \includegraphics[width=1\textwidth]{nm/fattree}
    \caption{Visual representation of the fat tree topology commonly used in data centers}
    \label{fig:fattree}
\end{figure}

\subsection {DCN Traffic}

\par So that solutions for network management in data-centers can be developed, first there needs to an understanding of the traffic characteristics and resource allocation, and utilize this information to shape the DC fabric.
The several studies proposed in traffic engineering for traditional networks do need to be revised in DCN's, since metrics like propagation delay, can be negligible, due to the physical proximity of nodes in DC's 
\cite{CITE - data_center_virt_survey}. However, studies on DCN's have proven difficult, since many data-center operators do not wish to publish information about their applications and services. Also contributing to this fact is
the impossibility of separating the different classes of data centers, since deployments across campus, private and cloud data-centers serve different purposes and have different applications.
\par By collecting data from different types of DC's, several studies have been made about the traffic characteristics \cite{ CITE - dc_networks_chars, CITE - dc_traffic_chars, CITE - fb_datacenter}:

\begin {itemize}
    \item The placement of VMs and servers effects the bandwidth and link capacity, due to the variety of applications that can be running on the servers at any time, and this non-uniform placement of VMs contributes to higher
        amounts of traffic originating from the same rack
    \item The majority of flows \footnote {flows are sequences of packets sent from a source to a certain destination (host, anycast or multicast domain)} are described as being small in size, and short in duration,
        which are usually described as \textit {mice} flows. 
        The counterpart to these are the \textit {elephant} flows, which occupy a very large share of the bandwidth, and degrade application performance, due to choking effect to the latency-sensitive mice flows. 
        Applications are tied to the type of traffic they generate, where online gaming, VoIP and multimedia broadcasting usually originate mice flows, where the large data transfers and file-sharing originate in elephant flows. 
        However, these flows which account for 80\% of total traffic only occur less than 10\% of total flows \cite {CITE - Broadcom Engineered Elephant flow for boosting ... }.
    \item In a normal situation, link utilization is low in the layers apart from the core switches. In addition to this discovery, losses are associated with spikes in traffic, instead being related to high utilization 
        of the link, which is one of the effects of the previously mentioned elephant flows.
\end {itemize}

\subsection {Limitations}

\subsection {DCN and SDN}

\par The centralized view that SDN controllers maintain over the networks allows for it to keep the information about the flows currently present in the network. As such, in the SDN paradigm, there is possibility to flexibly control 
the path that the packets take in the network, and improve performance of the network at a large scale. By joining the information available on DCN and SDN, the requirements for traffic engineering (TE) in SDN, from
the perspective of flow control are flow management, fault tolerance and traffic analysis \cite { CITE - traffic_engineering_sdn}. This set of four requirements set the base for properly monitoring a DCN from the 
perspective of the SDN paradigm.
\par The next section are taken from \cite { CITE - traffic_engineering_sdn}.

\subsubsection {Flow management}

Flow management refers to the capability that the controller has to set rules for packet forwarding, and maintain the low overhead that is associated with registering a new flow mod, and also limiting the amount of flow entries, 
as hardware switches usually have a set amount of flow entries that it can support. Further in this document, we explore the effect that the amount of flow entries has on OVS, where increasing the amount of flow entries also 
increases the packet loss between two hosts.
\par If we consider the fat-tree topology, then one obvious result is the fact that if one controller is responsible for the management of the entire underlying topology, then one possible results is the creation of one bottleneck
when the rules need to be deployed to a node. When the switch receives a new packet, and there are no rules to properly forward this packet, then the packet is redirected to the controller, on the form of a \textsc{PACKET\_IN} 
message, and after processing this packet a new flow mod is sent to the switch. The problem with this scenario lies in the delay that it takes between the reception of the packet, and the installation of the new
flow entry, which can be a contributing factor in packet losses in the data plane. This is an attack vector that is also explored in Distributed Denial of Service (DDoS) attacks for SDN platforms, as in an extreme scenario,
the spoofed packet addresses will not have matches on the tables, which then result on overflowing the controller \cite {early_detection_sdn_ddos}.
\par A solution for this issue is then related to decreasing the number of messages sent to the controller, by introducing some load balancing concepts. One of these concepts is related to the way that we can install the flow 
entries on the switch. The information present in the packets serve to generate the flow-match entries that are deployed on the table. To reduce the number of interactions between the controller and OF switches, then we can 
reduce the number of match fields present in the flow mods, which reduces the number of flow entries on the switch and the controller messages. Another solution is related to distribute the controller among the network, 
but keeping them connected via a separate channel.

\subsubsection {Fault tolerance}

Although the switches are connected in a way that are able to mitigate link, or other switch failures, in the case of faults occurring there needs to be the possibility of creation of new forwarding rules. An even bigger 
concern lies in the case when the controller fails, which will pose a larger problem in the network. For the case of node failure, fast recovery means that the OF controller can reactively react on link failures, by signaling the
switches to forward packets toward new locations; or proactively, by setting the rules prior to the occurrence of the failure. In the case that the failure is short lived, then the controller is also responsible of resetting the 
paths to the optimal state.
\par In the case of controller failover, then the backup controllers should act on this failure, and act as the new master. OF switches should connect to the set of available controllers, which should coordinate the management of 
the switch amongst themselves. After the switches first connection to the controllers, they should maintain this connection alive, but the controllers have the possibility of changing their roles. The controller roles are as follows:

\begin {itemize}
    \item \textsc {OFPCD\_ROLE\_EQUAL}, where the controller has full access to the switch, receiving all incoming messages, and can modify the state of the switch
    \item \textsc {OFPCD\_ROLE\_MASTER}, which is a similar status to the previous one, but where the switch ensures that only one switch is connected as the master role
    \item \textsc {OFPCD\_ROLE\_SLAVE} is a role that controllers has read-only access to the switch, having no permissions for altering the state of the switch. The only message that controllers registered with this role receive
        are the port-status messages
\end {itemize}

As previously mentioned, the way that controllers handle their connection is independent of the OpenFlow connection, and the failover should occur with minimal changes to the underlying flow rules and overhead.

\subsubsection {Traffic analysis}

So that the management tools can correctly display information about the state of the network, status statistics should be continuously collected and analysed. These statistics should provide the information about flows, packets
and ports, so that the measured metrics can serve as a baseline for the decisions of the controller to adapt the flow mods to enable the best possible performance. For the statistics collection there are two possible ways of 
getting the data: by continuously sampling packets from the switches; or applying sampling techniques, and generalizing the information from the sampled data \cite{CITE - low_overhead_te_elephants_detec}. The problem here lies
in the collection of the statistics in poses a problem for large scale deployments, where continuously polling the network devices introduces both overhead and very large amounts of data to be parsed, or the data is not enough
to detect failures in a short amount of time.

\section {Statistics}

\subsection {OpenFlow}

After exploring the requirements for network management, and the way the SDN model can support developing better systems, we now focus on the possibilites for obtaining this information from the networking devices. The OpenFlow 
protocol maintains a set of counters for each flow entry, port and group statistics, and this information can be queried to obtain a general view on the status of each OF switch. By sending specific controller-to-switch messages,
the switch will return a set of the maintained statistics, which can then be parsed and analysed further. 
\par Sending the port statistics message returns an array with the measured counters for each port. These counters include information like the amount of received and transmitted bytes/ packets, errors and dropped packets, and the
duration that the port is alive. 
\par The next important message is the \textsc{OFMP\_FLOW} message, since this allows for getting the individual flow statistics, and obtain the information about each flow entry, including the time that it has been set on the switch,
the number of packets/bytes in the flow, and the match fields. Also worth noting are the aggregate flow messages which describe how many packets are in the total flow entries, and also the number of flow entries that exist.
\par Also relevant is the information that is retrieved using the group statistics, as they allow to monitor the number of flows that direct to the group, and again the packet/ byte count that are matched with this group.
\par The information provided from these messages allows for a comprehensive view of the state of each switch, and a network management system (NMS) should utilize this information to achieve an understanding of the 
state of the network. 

\subsection {sFlow} \label {subsec:sflow} 

One problem arises, however, when periodic requests generate too much information, and the control channel is overflowed with messages of port statistics, which is a possible scenario when the
flow tables start getting too large. As such, a different alternative is to sample a small amount of packets from the switch, send the packet headers to the controller. One approach to this method is \textit{sFlow}, 
a standard for collection, analysis and storage of network flows and traffic, for each device and its interfaces. sFlow is implemented using embedded agents on switches and routers, which compile interface and flow samples and 
exports them to the sFlow collector via datagrams. 
\par Due to the problems that arise with continuously collecting traffic data, packet sampling has emerged as a valid solution to this problem, by collecting every \textit{n}-th packet randomly. The simplicity of the 
technique allows for reducing the complexity of sFlow agents, and having the sampling operation being done in hardware, resulting in the collection of the samples being done at the same speed of the channel it is monitoring. This
reduces the losses that are inherent to the sampling process, which leads to biased analysis of the traffic \cite {CITE - packet_sample_anomaly}.

\begin{figure} [!htbp]
    \centering
    \includegraphics[width=.6\textwidth]{nm/sflow_diagram}
    \caption{Architectural components of sFlow}
    \label{fig:sflow}
\end{figure}

\par Figure \ref{fig:sflow} shows the basic architecture that composes the sFlow system. One advantage of this system is the number of systems that incorporate sFlow agents \footnote {Complete list of compliant devices: 
http://www.sflow.org/products/network.php}, allowing for a detailed analysis of flows, and enabling flexibility for scalability in the network. By utilizing a sFlow collector that can accurately collect and process the datagrams 
incoming from the Agents, this protocol can be used to control most of the central aspects in network management, like troubleshooting network problems; controlling congestion on the network; or even analysing the possible 
security threats internal and external to the network.

\chapter {Berlin Institute for Software Defined Networks} \label{chap:bisdn} %% chapter 3

\section {Introduction}

\begin{figure} [!htbp]
    \centering
    \includegraphics[width=.4\textwidth]{bisdn/basebox}
    \caption{Basebox architecture}
\end{figure}

As the SDN market grows larger and larger in the networking world, new applications and products are developed and improved. Seeing the prevalence of closed source and proprietary solutions for this market, a need for open
products that enable further growth and innovation in cloud DCNs is evident. The main gain in moving from vertically integrated solutions, is the decrease of costs involved, as cheaper solutions can be found in whitebox 
\footnote {whitebox switches are} switches and open sourced networking applications. With this motivation, BISDN developed Basebox, a Linux-powered solution to integrate switches and SDN controllers, allowing for data center 
operators to configure and manage networks using linux commands, removing the need for having to manage several devices with different interfaces and workflows, and adding the capability of running standard networking applications 
on top of the controllers and switches. Basebox also includes the possiblity of running in a failover scenario, by introducing a backup controller for the network, and the possibility of creating a giant switch abstraction, 
by adding another controller, CAWR, and having this manage all the southbound switches.

\par In this chapter we focus on this product, on the first two sections some characteristics of the developed product are described, then we focus on the development of a management API, presenting the required technologies that 
were implemented, and then finally display the results that were obtained in this part of the thesis.

\section {Existing product}

\subsection {baseboxd}
\subsection {CAWR}

\section {Management API}

Due to the capabilities of Basebox of being a SDN controller used in data centers and a mission critical component for the network operators, it needed management capabilities, so that managing and operating infrastructure becomes 
an easier task. As such, the original problem presented was to build an interface extending the original work, so that the network statistics and the information of the topology could be easily displayed. There were several steps
then necessary to understand the problem, and be able to choose the best approach to this problem. The requirements for the proposed system were:

\begin {itemize}
    \item Display the topology information reported by CAWR, including the internal switch links, and the LACP discovered bond interfaces on the servers
    \item Display the port and link statistics for both switches
    \item Design an alerting system, so that network operators can be informed of changes on the network state
    \item Provide some diagnostic capabilities
\end {itemize}

\par The development of the work was the divided on two parts: the first part would be to implement the API needed to export the port statistics from baseboxd and CAWR, including a Graphical User Interface (GUI); and the second 
part was to study the alerting system, that would look into the statistics provided by the controllers, and design some rules so that QoS rules could be applied in the final product. This section describes the technologies needed 
to implement the API for Basebox.

\subsection {Data models}

Data models are abstract concepts that map the properties of entities and organizes their data, also defining how they relate to each other. To create a switch management interface, the entities we want to model are then 
the switches themselves, with attributes like the switch name, and the port counters, and the relationships of the data will allow us to display the links and topology of the network. One of the considerations that were taken into 
account when choosing the data models was the compatibility with standardized data models by the organizational entities like the IETF and the OpenConfig. 
\par The \textit{ NETCONF } network configuration model, which we explore further in \ref{ssec:netconf} also defines a data modelling language known as \textit{ YANG }, which is used in this protocol to model its configuration 
and data,and the remote procedure calls \cite { CITE - rfc 6020 }. By utilizing models defined in this language, the following condition is met: since this is a specific language for configuration and monitoring of networking 
devices, the existing data models will be similar to the ones that should be employed in the development of the management API. YANG data model defines the hierarchy of data between a NETCONF client and server with the objective
of smooth integration with the existing system's infrastructure. 
\par The systems we aim to model with these requirements are then two: the topology between the servers and switches, and the port statistics for each port one the switch. Since there is no data model that would accurately describe
both of them correctly, for the topology we chose the IETF network data model \footnote {https://github.com/YangModels/yang/blob/master/experimental/ietf-extracted-YANG-modules/ietf-network\%402017-12-13.yang}, and the 
OpenConfig interfaces data model \footnote {https://github.com/openconfig/public/tree/1040d11c089c74084c64c234bee3691ec70e8a9f/release/models/interfaces}, which contains the counters for each port.

\subsubsection {Topology}

The topology data model maps a collection of nodes, and the relationships between each node, called a link. This also allows for describing the network in a vertical hierarchy, by displaying relationships between several layers,
which can then be used to display the entire networking stack, for example displaying the physical links between nodes, their connections at layer 2 and layer 3 of the OSI model, and the virtualised relationships that the elements 
could have in a cloud deployement. As the development of the product continues, and more features are added, for example, layer 3 routing, then we require a flexible data model that can be extended to support the new capabilities.

\begin{figure} [!htbp]
    \centering
    \includegraphics[width=.3\textwidth]{bisdn/network_stack_topologies}
    \caption{Topology hierarchy achievable with this example \cite {CITE - https://www.ietf.org/id/draft-ietf-i2rs-yang-network-topo-20.txt}}
\end{figure}

\par Mapping the data model to the real world data is then adding the two types of information the data model expects: the first one composed of adding the different networks that composed the entire topology, including their
nodes and network types; and then using the previous information to build the links between each of the nodes, using the termination points the model exposes. In the implementation of the management API there was no need to implement 
underlying networks, but the extensibility this provides will be useful in the future.
\par Displaying the topology proved useful for CAWR, which provides the big switch topology, since this controller is directly connected to the underlying switches, and can see the links among these networking devices. The connection
to the bonded interface on the servers can also be monitored, since these can be configured to use LACP messages to report their status. To display the links between the switches, the information that LLDP provides is used, 
and if the controller is extended to be able to use LLDP to the servers, the further information can be filled into this data model, and provide a richer view on the status of each server.

\begin{figure} [h]
    \begin{subfigure}
    \includegraphics[width=0.5\textwidth]{bisdn/ietf_link}
    \end{subfigure}
    \begin{subfigure}
    \includegraphics[width=0.5\textwidth]{bisdn/ietf_node}
    \end{subfigure}
\caption{The IETF description for the nodes and links in the draft proposal for network topologies \cite {CITE - https://www.ietf.org/id/draft-ietf-i2rs-yang-network-topo-20.txt} }
\end{figure}

\subsubsection {Port statistics}

\par Modelling the port statistics to build a management interface requires first understanding of the OpenFlow statistics. As previously mentioned, OF switches maintain a set of counters, similar to SNMP, that provide information 
about the state of the ports, group, flow and table stats. The statistics that are exposed from OF are the following:

\begin{table}[]
    \centering
    \caption{OpenFlow port statistics}
    \label{my-label}
    \begin{tabular}{l | l || l | l}
       uint64\_t & rx\_packets     & uint64\_t & tx\_packets;     \\ \hline
       uint64\_t & rx\_bytes;      & uint64\_t & tx\_bytes;       \\ \hline
       uint64\_t & rx\_bytes;      & uint64\_t & tx\_dropped;     \\ \hline
       uint64\_t & rx\_errors;     & uint64\_t & tx\_errors;      \\ \hline
       uint64\_t & rx\_frame\_err; & uint64\_t & tx\_over\_err;   \\ \hline
       uint64\_t & rx\_crc\_err;   &                              \\ \hline
       uint64\_t & collisions;     &                              \\ \hline
       uint32\_t & duration\_sec;  &                              \\ \hline
       uint32\_t & duration\_nsec; &                 
    \end{tabular}
\end{table}

\par The chosen data model should then accurately model the fields that we need to expose, and the data type of counters we wish to measure. In this case, the prevalence of other controllers allows to use the same data models present 
in their implementations. OpenConfig \footnote{http://www.openconfig.net/} maintains a set of vendor neutral data models, written in YANG, allowing network operators to use standardized models for their networking infrastructure.
The entire set of published models can be accessed in their github page \footnote {https://github.com/openconfig/public}.

\subsection {Protocols}

None of the controllers had a clear way of obtaining the statistics apart from manually looking in the terminal and following the logs exposed and waiting for the appropriate output. There needs to be then a controllable, to export
this information and displaying them in a clear way. The solution was to develop a Graphical User Interface (GUI) for easily displaying the live statistics from the server, however there still was the problem of having to 
define the API that build the transport channel between baseboxd and CAWR to the GUI server. In this section we describe the two \textit {Remote Procedure Call (RPC)} systems that were researched, and focus on the advantages which
led to the final decision of implementing gRPC on Basebox.

\subsubsection {NETCONF} \label {ssec:netconf}

Despite it's dominance on network management products, SNMP features some bad characteristics that pose an obstacle for the widespread use in network configuration, and not only network management, like 
\cite {CITE - https://tools.ietf.org/html/rfc3535}: 

\begin {itemize}
    \item Incompleteness of the devices features
    \item SNMP access can sometimes crash systems, or return wrong data
    \item Unavailability of MIB modules, which forces users to use CLI's
    \item Poor performance 
    \item Security is difficult to handle
\end {itemize}

\par The IETF then, in light of this feedback obtained from network operators, started developing a protocol that allowed for the installation, manipulation and deletion of configuration of networking devices called NETCONF, which 
enables devices to expose a full API to their systems. This protocol, based in client/ server communication and is based in the four layers, as can be seen in the following image:

\begin{figure} [!htbp]
    \centering
    \includegraphics[width=.4\textwidth]{bisdn/netconf}
    \caption{NETCONF protocol layers \cite {CITE - Basebox architecture}}
\end{figure}

\par Data models and operations, covered in detail in the previous section, is related to the Content layer on the image, so this will not be covered in this section. 
\par Configuration of a network device can be complex, and managing separate configurations between device startup and normal operation is a difficult task, but there is occasional need for this capability. NETCONF defines the 
existence of different \textit{datastores} to enable this feature, allowing the network operator to set an initial configuration, used when the device is initialized, and switching to the running datastore when the device is ready
to maintain normal operation. This concept of datastores also enables the creation of a candidate datastore, providing the capability of testing configurations on the network device, checking for any possible errors, while making
sure that there is no impact on the current configuration of the device. After the changes have been tested and validated, a <commit> operation can be used to deploy the new configuration to the running datastore.
\par Another useful feature that is described in the NETCONF protocol, is the possibility of using the rollback-on-error capability. When rolling a new change, and if the system is enabled to support this feature,
NETCONF can detect errors in the changes done to the configurations, and return the system to the previous state that is error free. 
\par The NETCONF API provides several operations to interact with the managed devices to get system information and push new configurations. The set of supported operations in the base NETCONF protocol can be accessed in 
\url{https://tools.ietf.org/html/rfc6241#section-7}. 
\par In regards to the transport layer, NETCONF is able to run on top of several protocols. However, NETCONF requires that a persistent connection is maintained between devices, and this connection should be reliable, and support
transmission failure. In addition, the security should be handled by the transport layer \cite { CITE - https://www.ietf.org/slides/slides-edu-netconf-yang-00.pdf}, providing the guarantee that transactions are done in a 
cryptographically secure channel, between two authenticated hosts. As a results, typical NETCONF implementations are based on SSH or TLS protocols.

\subsubsection {gRPC}  \label {ssec:grpc}

The basic idea behind the RPC system is defining services by setting the interaction between remote systems, allowing for directly calling objects on remote systems. Based in the client/server communications 
pattern, gRPC allows for interactions between different environments, even implemented with different programming languages, all based on the same data structure. This data structure can be serialized using another modelling 
language, called \textit {protobuf}, which will define the data, which is defined as messages, and the services that contain the RPC calls between systems. Since this system is based on the HTTP2 transport layer, we are able to
use the advantages that this protocol provides us.
\par Despite the serialization language used in gRPC is based on protocol buffers, unlike YANG, there are some projects \footnote {https://github.com/openconfig/goyang} that enable the translation between YANG to protofile,
which allows us to use the data models we chose, only adding one extra step to convert the files.
\par Despite both protocols capability of meeting the requirements that were presented to us, the gRPC framework was chosen due to several reasons:

\begin {itemize}
    \item Both frameworks allow us to use the standardized data models currently proposed by the IETF and OpenConfig
    \item NETCONF trades information as XML encoded information, for both the edit and get config operations; while gRPC allows to handle information in a way that’s native to the language implementation of the client/server
    \item The integration with the existing system was easier: since gRPC has implementations for the languages that the controllers are developed on (i.e. C++/ Python), this framework was easier to implement 
        than NETCONF, which would have required integration with third party tools, or a longer development cycle to make sure that the developed applications would met the requirements
\end {itemize}

\section {Results}


