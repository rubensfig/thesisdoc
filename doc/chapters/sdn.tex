\section{Software Defined Networking} \label{chap:sdn}

Computer networking is a vital part of the services that are offered today, and as such, the performance in technology backing these is central to the
quality of these services. As the service providers move their data centers to cloud computing environments, enabling several improvements in the predictability,
Quality of Service and ease of use of their services, new technologies are required to make sure that their services are adapted to the fast changing landscape of 
networking. One of the most notable innovations in this field is called \textbf{Software Defined Networking}, which, as described by the Open Networking
Foundation, \textit{ In the SDN architecture, the control and data planes are decoupled, network intelligence and state are logically centralized, and the underlying
network infrastructure is abstracted from the applications} \cite{open_networking_foundation_sdn_2014}. 
The two main contributions of this architecture is:

\begin {itemize}
    \item \textbf{Separation of data and control planes} SDN allows for the separation of the network control plane from the data forwarding plane by having network
        "intelligence" present in the network controllers, and having them control the forwarding elements that live in the Data Plane
    \item \textbf{Centralization of network management functions} By isolating the management on a separate plane, there is possibility of developing a single
        controller that can regulate the entire network, having unrestricted access to every element present in the network, simplifying management, monitoring,
        application of QoS policies, flow optimization, etc
\end {itemize}

\par This new paradigm introduces programmability in the configuration and management of networks, by consolidating the control of network devices to a single
central controller, achieving separation of the control and the data plane, and supporting a more dynamic and flexible infrastructure. Another important development
following the development of SDN is the concept of Network Function Virtualisation (NFV). This concept allows to remove \textit {middleboxes}
\footnote {Computer networking device that does some operations on traffic, besides packet forwarding. Examples include caches, Intrusion Detection System (IDS),
Network Address Translation (NAT), ..}, by replacing these with generic software applications.

\begin{figure}[H]
  \centering
  \subfloat[Traditional networking architecture]{\includegraphics[width=0.4\textwidth]{bib/network_trad}\label{fig:net_trad}}
  \hfill
  \subfloat[SDN architecture]{\includegraphics[width=.4\textwidth]{bib/network_sdn}\label{fig:net_sdn}}
  \caption {Traditional vs SDN network architecture}
\end{figure}

\par By moving network infrastructure to SDN models, the difficulty of managing a network is greatly reduced, since the logical centralization of the control layer
exposes the global view of the network, simplifying management tasks. Furthermore, this also removes the challenge of configure each networking device individually,
turning network operation and management into setting high level policies in the controllers, and letting the protocols that handle connection between the devices
and controllers set the actual rules. 

\par Software-Defined Networking is defined as being composed of two layers: \textbf{Northbound Interfaces}, which are composed of Application Programming Interfaces
(API) for communication between applications and the controller, enabling network services like routing, security, visualization and management; and the
\textbf {Southbound Interfaces} which connect the network devices to the controllers via protocols like OpenFlow (see section \ref{sec:of}), or P4
\footnote {https://p4.org/}.

\begin{figure}[!tbph]
  \centering
  \includegraphics[width=0.4\textwidth]{sdn/sdn_division}
  \label{fig:sdn_div}}
  \caption {SDN Interfaces division}
\end{figure}

\par Understanding SDN platforms is then composed of understanding the operation of both interfaces, and defining requirements for their operation, which are
listed below \cite{shin_software-defined_2012}.
These requirements are general principles for networks, but the addition of the SDN controller introduces a single point of failure, that could be damaging to the 
entire network.

\begin {enumerate} 
    \itemsep0em
    \item High Performance   
    \item High Availability 
    \item Fault Tolerance   
    \item Monitoring   
    \item Programmability   
    \item Modularity   
    \item Security   
\end {itemize}

\subsection {OpenFlow} \label{sec:of}

With the growth of the networking infrastructure of the past few decades, the need for an environment that allows for experimentation and testing of different
protocols and equipment became evident. If networking research would depend on the previously existing methods, then new ways of creating and developing protocols
would become increasingly hard to implement and develop. As such, there was need for a framework that could enable testing of new ideas on close to realistic
settings. So, on February 2011 the version 1.1 of OpenFlow was released, and this proposal quickly became the standard for networking in a Software Defined Network.
Since 2011, this protocol has suffered some revisions, and the latest version released is version 1.5.1. 

\par Several reasons led to the quick standardization of this protocol, which are related not only to the initial requirements of the platform, like the capability
of supporting high-performance and low-cost implementations, but also the extensibility that the open source development model provides, removing the limitations
that typical commercial solutions give the network researchers.

\par The big advantage of OpenFlow is that it is, from the data forwarding point of view, easy to process, since the control decisions are made by the controller
present in a separate plane, and all the switch needs to do is correctly match the incoming packets, forwarding them according to the rules established by the
controller. The components that are part of this system and enable this functionality are:

\begin {itemize}
    \item \textbf {Flow Tables} This element describes the main component of the switching capabilities of the OpenFlow switch. Inside the switch there are several
        flow tables that contain rules to match incoming packets, and process them according to the rules specified by the controller. These rules can
        contain actions that affect the path of the packets, and these actions usually include forwarding to a port, packet modification, among others.
        Classification is done via matching one or more fields present in the packet, for example the switch input port, the MAC and IP addresses, IP protocol.
        The required actions for an OpenFlow switch are the capability of forwarding to a set of output ports, allowing the packet to move across the network;
        to send them to the controller, in the case of a miss of match; and finally the ability to drop packets, which is useful for DDoS mitigation, or other
        security concerns.
    \item \textbf {OpenFlow Protocol} The OpenFlow Protocol between the switch and the controller defines several messages that allow for the control of the switch.
        This protocol enables capabilities such as adding, deleting and updating flow rules in the switch, referred to as \textit {Controller-to-Switch}
        messages. Other relevant message types are the \textit {Asynchronous} messages, that provide notifications of events that occurred. This type includes
        the PACKET\_IN message, which is a type of message that is sent to the controller when a certain packet has no match in
        the flow tables present in the switch. Finally, the \textit{Synchronous} message, like the Hello message, which are used for negotation of the OpenFlow
        version and other elements to help connection setup. The initial connection is initialized by either the switch or the controllers over the defined
        transport protocol, but after the transport connection is established, the OpenFlow channel should behave the same way 
        \cite{open_networking_foundation_openflow_2015}.
    \item \textbf {Secure Channel} OpenFlow defines the channel that is between the switch and the controller as a secure communications channel. As messages 
        that are exchanged between the switches and the controllers are critical for the correct operation of the system, the channel should be cryptographically
        secure, to prevent spoofing and manipulation of this information. As such, the channel is usually transported over TLS.
\end {itemize}

\begin{figure} [h]
    \centering
    \begin{subfigure}
    \includegraphics[width=0.25\textwidth]{sdn/open_flow_switch_pipeline}
    \end{subfigure}
    \begin{subfigure}
    \includegraphics[width=0.6\textwidth]{sdn/open_flow_tables}
    \end{subfigure}
    \caption{Images describing OpenFlow components. On the left, an overview to the entire system, and on the right a view at the table structure of the 
    OpenFlow Switch \cite{open_networking_foundation_openflow_2015}}
    \label{fig:of_switch}
\end{figure}

\par Figure \ref{fig:of_switch} describes the OpenFlow components. The image on the left shows the entire pipeline and the connections between the controllers and
switch, but lacks the connection of the OpenFlow switch to the data plane switch ports. In image on the right, the structure of the Flow Table is summarized, 
and the packet in originates from the switches' ingress port, and after processing, the packet will exit through the switches' egress port.

\par In the case of controller failover, then the backup controllers should act on this failure, and act as the new master. OF switches should connect to the set of
available controllers, which should coordinate the management of the switch amongst themselves. After the switches first connection to the controllers, they should
maintain this connection alive, but the controllers have the possibility of changing their roles. The controller roles are as follows:

\begin {itemize}
    \item \textsc {OFPCD\_ROLE\_EQUAL}, where the controller has full access to the switch, receiving all incoming messages, and can modify the state of the switch
    \item \textsc {OFPCD\_ROLE\_MASTER}, which is a similar status to the previous one, but where the switch ensures that only one switch is connected as the master
        role
    \item \textsc {OFPCD\_ROLE\_SLAVE} is a role that controllers has read-only access to the switch, having no permissions for altering the state of the switch.
        The only message that controllers registered with this role receive are the port-status messages
\end {itemize}

The OpenFlow switches maintain a set of counters, similar to SNMP, that provide information about the state of the ports, group, flow and table stats. The
statistics that are exposed from OF are shown in table \ref{tab:of_port_stats}.

\begin{table}[H]
    \centering
    \caption{OpenFlow port statistics}
    \begin{tabular}{c | c || c | c}
       uint64\_t & rx\_packets     & uint64\_t & tx\_packets;     \\ \hline
       uint64\_t & rx\_bytes;      & uint64\_t & tx\_bytes;       \\ \hline
       uint64\_t & rx\_bytes;      & uint64\_t & tx\_dropped;     \\ \hline
       uint64\_t & rx\_errors;     & uint64\_t & tx\_errors;      \\ \hline
       uint64\_t & rx\_frame\_err; & uint64\_t & tx\_over\_err;   \\ \hline
       uint64\_t & rx\_crc\_err;   &                              \\ \hline
       uint64\_t & collisions;     &                              \\ \hline
       uint32\_t & duration\_sec;  &                              \\ \hline
       uint32\_t & duration\_nsec; &                 
    \label{tab:of_port_stats}
    \end{tabular}
\end{table}

\subsubsection{OpenFlow Data-Path Abstraction} \label{sec:ofdpa}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{sdn/ofdpa_layers}
    \caption{OF-DPA components \cite{broadcom_corporation_openflow_2017}}
    \label{fig:ofdpa_struct}
\end{figure}

OpenFlow Data-Path Abstraction (OF-DPA) allows development of OpenFlow based SDN applications based in Broadcom's hardware switches. It provides a hardware 
abstraction layer, supporting programming of network devices using OpenFlow. The main difference from pure OpenFlow to Broadcom's implementation are the Flow
Table structures, known as the Table Type Patterns (TTP). These TTPs are templates that describe the protocol features and messages that the controllers and switches
need to support, providing developers additional structure for easier implementation of their applications. Furthermore, the specialized structure provided by these
patterns allow for optimizing the allocation of table memory, and improve the lookup algorithms.

\par Figure \ref{fig:ofdpa_tables} displays the utilized TTP in OF-DPA.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{sdn/ofdpa_ttp}
    \caption{OF-DPA TTP \cite{broadcom_corporation_openflow_2017}}
    \label{fig:ofdpa_tables}
\end{figure}

\subsection {Network Devices}

Networking devices are a central part of network operation, performing routing, switching, management operations, and span the different layers of the OSI model. 
The investment in dedicated hardware to perform management functions can potentially be replaced by SDN controllers, offloading the QoS policies and traffic
engineering (TE) functions from hardware to software. As such, in this section we focus on the devices that are responsible for the operation on layers 2 and 3 of
the standardized networking stack, switches and routers. A networking switch is a device that connects multiple devices on a computer network, using hardware
addresses (MAC) to forward data inside the network, by mapping each port with a certain MAC address, while a router is responsible of forwarding packets between
different computer networks. These devices run at different layers of the networking stack, the former operating at the data-link, or layer 2, and the latter
operating at layer 3, or network. This is not a clear separation however with multilayer devices, where switches also provide routing capabilities. Typical vendors
for these solutions include Cisco and Juniper, but the rise of whitebox switches that have support for deployments in SDN environments enables network operators to
avoid vendor lock-in, and take advantage of the open nature of these devices. Commonly associated with whitebox switches is the support for the OpenFlow protocol,
making these an essential part of the SDN infrastructures.

\par The performance \footnote {Defined as the throughput and latency of the network node} of networking devices is central to the proper operation of networks,
especially in deployments in Data Centers, where the interfaces must be able to support 100 Gbps links and further, while also maintaining the programmability that
is expected of SDN based infrastructures. This performance is linked with the hardware that is chosen to serve as the base for the devices \cite{sezer_are_2013}:

\begin{enumerate} 
    \item \textbf{General Purpose Processors (GPPs)} provide the greatest flexibility of all the solutions, while providing the worst results in performance, 
        due to the general purpose design of the hardware and the optimizations present in the other architectures.
    \item \textbf{Field-Programmable gate arrays (FPGA)} are a platform that enables the configuration of the devices via hardware design tools, maintaining the
        programmability of the GPPs, while also allowing for designing the devices around the tasks that they perform, including optimizations for switching/ routing.
        A notable example of platforms based in these systems is NetFPGA \footnote {https://netfpga.org/site/\#/}, an open source hardware platform designed for
        research, and supporting up to 100 Gbps operation. 
    \item \textbf{Application-specific integrated circuits (ASIC)} are integrated circuits that are customized for one particular application, removing the
        programmability, but also providing the greatest performance of the former options.
\end {itemize}

\par These architectures generally allow to design SDN architectures around general purpose hardware, contributing to the flexibility of this paradigm, even
considering the proprietary nature of the ASICs, which can be bundled with Software Development Kits (SDKs) for developing other
applications on top of these. 

\par Considering OF enabled hardware switches, the processing of incoming packets is done as by matching a (up to) 15 field tuple \cite{raz_2014_2014} to several 
flow tables, that have rules sent from the controller. In these cases, the possibility of bottlenecks are due to several factors, including the latency
of the installation of new flow rules, and the memory limitation on the hardware. Solutions to the memory limitations in OF switches include DevoFlow 
\cite{nunes_survey_2014}, which utilizes wild card rules to reduce the number of flow entries that are installed on the devices, while also aggregating 
traffic, simplifying detection and management of unexpected large volumes of traffic (see section \ref{sec:el_fl}), due to reduced control plane load; and SmartTime
\cite{vishnoi_effective_2014} manages the timeouts for the rules on the switch, reducing this in the presence of micro flows, and increasing the timeout
in the case of the occurrence of longer lived flows, which improves memory utilization and reduces the load on the controller.

\par Virtualised environments also allowed the development of Software Switches, due to the highly dynamic nature of virtual environments with frequent network 
topology changes caused by Virtual Machine (VM) movement between physical hosts.  Furthermore, standard Linux bridges cannot handle the multi-server
deployments \footnote {https://github.com/openvswitch/ovs/blob/master/Documentation/intro/why-ovs.rst} used in virtualised environments, which is why software 
switches like Open vSwitch (OVS) replace traditional switches in multi-server virtual deployements. OVS switches are also compliant with the OpenFlow
protocol, which shows the flexibility that can be achieved by combining all the networking devices with one management protocol. 

\subsection {SDN Controllers}

Central for operation of the networks, SDN controllers allow the orchestration of the multiple parts required for correctly operating a large scale network.
Separate from the data traffic on the network, these are responsible for the interaction between the Northbound networking applications and the Southbound
devices, as described in figure 2.4.
%\ref{fig:sdn_interf_xx}.

\begin{figure}[!tbph]
  \centering
  \includegraphics[width=0.5\textwidth]{sdn/sdn_controller_arch}
  \label{fig:sdn_interf_xx}
  \caption {SDN Interfaces division}
\end{figure}

\par Although the use case of the controllers will depend on each deployment and implementation, the basic use case is to provide connectivity across layer 2 and 3
networks, which is achieved via flow management, including operations like switching, forwarding and potentially load balancing. The logically centralized
position augments this capability by keeping the state of the entire network, which facilitates route planning and management. 

\par Despite the advantages that the controllers centralization provides, this also introduces a Single Point of Failure (SPOF) \cite{phemius_disco:_2014}, exposing
a weakness to Denial-of-Service (DoS) attacks and controller failure. The potential catastrophic scenario related to controller downtime due to these
failures means that an approach must be planned for disaster failure and recovery. High Availability setups are used to mitigate the potential failure of
the controllers, by having multiple backups running. In order to guarantee that all controllers see the same network state, every switch must be connected to 
every controller, but as specified in section \ref{sec:fault_tolerance}, only the Master controller writes the messages to the networking devices, which
ensures that duplicate rules are not enforced. In case of controller failure, one of the backup controllers can take over the role of the previous master,
without any outages on the network.

\subsubsection{Existing Platforms}

There are several controller implementations available for use, each with different interfaces, performance, and modularity. In \cite{khondoker_feature-based_2014} a
comparative study is performed on available SDN controllers in 2014, and compare the different characteristics of each controller, like the available
interfaces, the language of implementation, modularity, etc. In the following sections, we explore two of the highest rated solutions, Floodlight and
OpenDaylight.

\subsubsection {Floodlight} \label{chap:flood}

Floodlight \footnote{http://www.projectfloodlight.org/floodlight/} is a java-based SDN controller, and is one the first open-source solutions to gain relevance in
research and industry \cite{berde_onos:_2014}. The Northbound endpoint provides a REST API (see figure \ref{fig:flood_arch}) to interact with the switch, which
allows developers to get statistics, push flow entries, and more. 

\begin{figure}[H]
  \centering
  \includegraphics[width=.3\textwidth]{sdn/floodlight_architecture}\label{fig:flood_arch}
  \caption {Floodlight architecture \cite{project_floodlight_floodlight_2017}}
  \label{fig:flood_arch}
\end{figure}

\par This controller also provides the OpenFlow interface, and it enables adding several modules, either through extensions, or through the utilization of the
provided REST interface, simplifying the addition of new features to the base controller. It also provides an useful GUI for easier visualization of the topology,
link state, and port statistics, which is visible in figure \ref{fig:flood_gui}.
 
\begin{figure}[H]
  \centering
  \includegraphics[width=.8\textwidth]{sdn/floodlight_topology}
  \label{fig:flood_gui}
  \caption{Floodlight web display of the topology}
\end{figure}

\subsubsection {OpenDaylight} \label{chap:odl}

\begin{figure}[H]
  \centering
  \includegraphics[width=.5\textwidth]{sdn/odl_arch}
  \label{fig:odl_arch}
  \caption {OpenDaylight controller architecture \cite{medved_opendaylight:_2014}}
\end{figure}

OpenDaylight \footnote{https://www.opendaylight.org/} (ODL) is a project supported by the major vendors. The main differences between ODL and other controllers
is support for other protocols in the southbound interface, due to the creation of a Service Abstraction Layer (SAL) \cite{medved_opendaylight:_2014}. In a high 
level overview, the creation of the Model-Driven SAL allows to extend the controller basic functionality by the addition of several plugins, which are used in
combination with a RESTCONF \cite{bierman_restconf_2017} interface, defining the data models for the data stores, and the RPCs for interaction between the data.

\begin{figure}[H]
  \centering
  \includegraphics[width=.9\textwidth]{sdn/odl_topology}
  \label{fig:odl_topo}
  \caption {OpenDaylight Topology}
\end{figure}

\subsubsection {Data center monitoring with SDN}

\par The centralized view that SDN controllers maintain over the networks allows for it to keep the information about the flows currently present in the network. As
such, the SDN paradigm allows for flexible control of the path the packets take in the network, and improves performance of the network at a large
scale. By joining the information available on DCN and SDN, the requirements for traffic engineering (TE) in SDN, from the perspective of flow control are flow 
management, fault tolerance and traffic analysis \cite{akyildiz_research_2016}. This set of four requirements set the base for properly monitoring a DCN from the 
perspective of the SDN paradigm.
\par The next section are taken from \cite{akyildiz_research_2016}.

\subsubsection {Flow management}

Flow management refers to the capability that the controller has to set rules for packet forwarding, and maintain the low overhead that is associated with
registering a new flow rule, and also limiting the amount of flow entries, as hardware switches usually have a set amount of flow entries that they can support.

\par If we consider the fat-tree topology, one obvious consequence is the fact that if one controller is responsible for the management of the entire underlying 
topology, a bottleneck can be created when the rules need to be deployed to a node. When the switch receives a new packet, and there 
are no rules to properly forward this packet, then the packet is redirected to the controller, on the form of a \textsc{PACKET\_IN} message, and after processing
this packet, a new flow rule is sent to the switch. The problem with this scenario lies in the delay that it takes between the reception of the packet, and the
installation of the new flow entry, which can be a contributing factor in packet losses in the data plane. This is an attack vector that is also explored in
Distributed Denial of Service (DDoS) attacks for SDN platforms, as in an extreme scenario, the spoofed packet addresses will not have matches on the tables, which
then result on overflowing the controller \cite{mousavi_early_2015}.

\par A solution for this issue is then related to decreasing the number of messages sent to the controller, by introducing some load balancing concepts. One of
these concepts is related to the way that we can install the flow entries on the switch. The information present in the packets serve to generate the flow-match
entries that are deployed on the table. To reduce the number of interactions between the controller and OF switches, then we can reduce the number of match fields
present in the flow rules, which reduces the number of flow entries on the switch and the controller messages. Another solution is distributing the controller among
the network, but keeping them connected via a separate channel.

\subsubsection {Fault tolerance} \label{sec:fault_tolerance}

Although the switches are connected in a way that are able to mitigate link or other switch failures, in the case of faults occurring there needs to be the
possibility of creation of new forwarding rules. An even bigger concern lies in the case when the controller fails, which will pose a larger problem in the network. 
For the case of node failure, fast recovery means that the OF controller can reactively act on link failures, by signaling the switches to forward packets toward 
new locations; or proactively, by setting the rules prior to the occurrence of the failure. In the case that the failure is short lived, then the controller is also 
responsible of resetting the paths to the optimal state. The way that controllers handle their connection is independent of the OpenFlow connection, and the
failover should occur with minimal changes to the underlying flow rules and overhead.

\subsubsection {Traffic analysis}

So that the management tools can correctly display information about the state of the network, status statistics should be continuously collected and analysed. These
statistics should provide the information about flows, packets and ports, so that the measured metrics can serve as a baseline for the decisions of the controller to
adapt the flow rules to enable the best possible performance. The statistics collection can be collected in two possible ways: by continuously sampling
packets from the switches; or applying sampling techniques, and generalizing the information from the sampled data \cite{curtis_mahout:_2011}. The problem here lies
in the collection of the statistics in poses a problem for large scale deployments, where continuously polling the network devices introduces both overhead and very 
large amounts of data to be parsed, or the data is not enough to detect failures in a short amount of time. 

\subsection {Statistics}

\subsubsection {OpenFlow}

After exploring the requirements for network management, and the way the SDN model can support developing better systems, we now focus on the possibilities for
obtaining this information from the networking devices. The OpenFlow protocol maintains a set of counters for each flow entry, port and group statistics, and this
information can be queried to obtain a general view on the status of each OF switch. By sending specific controller-to-switch messages, the switch will return a set
of the maintained statistics, which can then be parsed and analysed further. 

\par Sending the port statistics message returns an array with the measured counters for each port. These counters include information like the amount of received
and transmitted bytes/ packets, errors and dropped packets, and the duration that the port is alive. 

\par The next important message is the \textsc{OFMP\_FLOW} message, since this allows for getting the individual flow statistics, and obtain the information about
each flow entry, including the time that it has been set on the switch, the number of packets/bytes in the flow, and the match fields. Also worth noting are the 
aggregate flow messages which describe how many packets are in the total flow entries, and also the number of flow entries that exist.

\par Also relevant is the information that is retrieved using the group statistics, as they allow to monitor the number of flows that direct to the group, and again
the packet/ byte count that are matched with this group.

\par The information provided from these messages allows for a comprehensive view of the state of each switch, and a Network Management System (NMS) can utilize
this information to achieve an understanding of the state of the network. 

\subsubsection {sFlow} \label{sec:sflow}

One problem arises, however, when periodic requests generate too much information, and the control channel is overflowed with messages of port statistics, which is
a possible scenario when the flow tables start getting too large. As such, a different alternative is to sample a small amount of packets from the switch, send the
packet headers to the controller. One approach to this method is \textit{sFlow}, a standard for collection, analysis and storage of network flows and traffic, for 
each device and its interfaces. sFlow is implemented using embedded agents on switches and routers, which compile interface and flow samples and exports them to the 
sFlow collector via datagrams. 

\par Due to the problems that arise with continuously collecting traffic data, packet sampling has emerged as a valid solution to this problem, by collecting every
\textit{n}-th packet. The simplicity of the technique allows for reducing the complexity of sFlow agents, and having the sampling operation being done in 
hardware, resulting in the collection of the samples being done at the same speed of the channel it is monitoring. This reduces the losses that are inherent to the
sampling process, which leads to biased analysis of the traffic \cite{brauckhoff_impact_2006}.

\begin{figure} [!htbp]
    \centering
    \includegraphics[width=.6\textwidth]{nm/sflow_diagram}
    \caption{Architectural components of sFlow}
    \label{fig:sflow}
\end{figure}

\par Figure \ref{fig:sflow} shows the basic architecture that composes the sFlow system. One advantage of this system is the number of systems that incorporate sFlow
agents \footnote {Complete list of compliant devices: http://www.sflow.org/products/network.php}, allowing for a detailed analysis of flows, and enabling flexibility
for scalability in the network. By utilizing a sFlow collector that can accurately collect and process the datagrams incoming from the Agents, this protocol can be
used to control most of the central aspects in network management, like troubleshooting network problems; controlling congestion on the network; or even analysing
the possible security threats internal and external to the network.

\pagebreak
