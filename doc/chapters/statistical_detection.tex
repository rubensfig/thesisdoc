\section{Time series analysis} \label{chap:stat_det}

As a result of the large scale of current data centers, maintaining control over these networks proves a difficult task. Networks operators must then adapt to the
current situation by improving the monitoring infrastructures to allow faster response to problems, and \textbf{Root-Cause Analysis} (RCA) \footnote{method of
identifying the initiating cause of an error or fault in a system} of the source of network issues can be done faster and easier, which will reflect on better 
service and lower costs for network operators. 

\par Network behaviour analysis is defined by the constant monitoring of a network, so that events that compromise the "healthy" state of the network can be
removed or mitigated. These include not only cases where the anomalies are caused with malicious intent, like the case of DDoS attacks, but also failure of network 
devices or changes in user behaviour \cite{munz_traffic_2010}. These systems are equipped with alarm capabilities, so that system administrators can quickly respond
to changes, possibly even giving some information about the source of the problem. However, the automation of these monitoring processes means that the possible 
existence of false alarms reduces the operators capabilities to act on actual failures. 

Understanding processes and their results is a key factor in the success of implementing new features, or analysing existing ones for their efficiency and output.
This analysis, important for the different fields in engineering, economics, health, allows obtaining information about the normal and abnormal state of each 
underlying process, provide forecasts and predictions on short and long term behaviour of the relevant data, classify and cluster information, and more. The act of 
collecting and processing the data, over a period of time is called \textit{time series analysis}. 

\par Monitoring systems provide a guarantee in quality engineering, since they allow to follow a system and its properties, and notify operators if changes happen
that impact the normal status of a relevant parameter. These changes can be occasional or systematic, but should the state of the system deviate from the limits set
by the operators, researching the cause of the errors can reveal some errors or malfunctions in the system. \textit{Change detection} is the study of the different 
parameters of the system, and determining the points where these cause a significant deviation from the normal operation. In network traffic analysis, these methods
can be applied to determine when the behaviour of certain flows, that can be monitored over metrics originating from the controller and switches, impact the traffic
characteristics. One key part on the application of changepoint detection is the understanding and selection of the proper metrics to monitor, to ensure that these
are sensitive to the traffic changes. One other important consideration in applying changepoint detection mechanisms is the reduction of false alarms, that occur 
when the metrics are too sensitive to traffic changes, and limit the network operators capability of accurately responding to real network issues. 

\par Change detection mechanisms are classified as follow  \cite{munz_traffic_2010}:

\begin {itemize}
    \item In change detection theory, an important distinction is the difference between online and offline detection. Offline, or batch detection 
        methods consider a fixed length of observations, and retrospectively analyse the dataset to determine the time where the change, or changes took place. 
        Online detection, or sequential detection, unlike batch detection which uses all the available observations to detect the changes, including the
        ones obtained after the change took place, is based on the determination of the change points based on the arrival of the new data, allowing for determining
        the change as fast as possible \cite{ahmed_novel_2008}. 
    \item Another important distinction is related to the scalability, and the amount of data that needs to be stored to accurately implement
        detection on new samples. Parametric approaches rely on learning a probability distribution from the monitored variables, and using this learned data to 
        estimate the unknown parameters, after which the training data can be discarded. Non-parametric models however, do not take into consideration the
        distribution of the monitored variables, and analyse statistical properties instead. The cost of this analysis is that the previous data must be stored to
        provide better results, but this problem can be mitigated using algorithms like sliding windows or moving averages.
\end {itemize}

\subsection{Mathematical formulations} \label{sec:math_form}

A time series can be defined as a stream of observations $X = \{x_1, ... x_i\}$, where $x_i$ is a vector arriving at time $i$. The time series $X$ can also be
described as the sum (in equation \ref{eq:ts_sum}), or product (in equation \ref{ts:pr}) of the following components: $S_t$, which refers to the seasonal component
of the data; $T_t$, which defines the trend of the data, and $R_t$ represents the residual values, accounting for non expected variation and noise.

\begin {equation}
\centering
X = S_t + T_t + R_t
\label{eq:ts_sum},
\end {equation}

Or:

\begin {equation}
\centering
y_t = S_t * T_t * R_t
\label{ts:pr}
\end {equation}

\par In regards to the classification of the trends according to the type of change, they can be classified as:

\begin {itemize}
  \item \textbf {Deterministic} when the trend consistently increase/decrease
  \item \textbf {Stochastic} when the opposite happens
\end {itemize}

\par Time series data usually present a non stationary behaviour, that are characterized by changes in the mean and variance. Statistical methods require, however,
stationarity in the data. The presence of trends and cyclic behaviours is the most common violation of stationary, and table \ref{tab:trends_here} shows the most 
common trends present in the data, and the parameters that need to be learned from the time series in study. 

\begin{table}[h]
\centering
\begin{tabular}{c|c}
Linear      & $y = m \cdot x + b$ \\ \hline
Polynomial  & $y = b + c_1 \cdot x + \dotso + c_n \cdot x^n$  \\ \hline
Exponential & $y = c \cdot  x^b$     \\ \hline
Logarithmic & $y = a \cdot ln(x) + b$
\end{tabular}
\caption{Trend models}
\label{tab:trends_here}
\end{table}

Removing systematic changes like trends is also possible with differencing,

\begin {equation*}
\centering
\nabla X_t = X_t - X_{t-1}.
\end {equation*}

\subsection{Forecasting} \label{sec:forecasting}

Time-series' have the possibility of applying statistical models to extract the next value prediction based on past observations. Under the assumption that the
underlying process can be modeled by previous historical values, and assuming this model remains true for future measurements, the time series data historical 
behaviour can be used to generate these forecasts for future values in the time series. 

\subsubsection{Exponential smoothing}

\par Exponential smoothing allows for generating predictions using the historical behaviour, by applying a set of weights to the data that exponentially decreases 
over time. Considering the time series $X_t$, the one-step ahead prediction $\hat{x}_t$ can be obtained by equation \ref{eq:exp_smooth}.  In this model, the 
smoothing factor $\alpha (0 < \alpha < 1)$ should be obtained empirically, and its value will determine the forgetting factor for the past observations.

\begin {equation*}
    \centering
        \begin {split}
            &\hat{x}_1 = x_0, \\
            &\hat{x}_t = \alpha x_{t} + (1-\alpha)x_{t-1}, t > 1.
        \end {split}
    \label{eq:exp_smooth}
\end {equation*}

\par Due to its simplicity, this method is not suitable for situations where the data has trends, or seasonal behaviours \cite{kalekar_time_2004}. The solution for
this problem is introduced with double exponential smoothing, also know as Holt forecasting, and triple exponential smoothing, also known as Holt-Winters
forecasting. These methods introduce further components to dampen the effects of cyclic behaviours in the data. Double exponential smoothing is defined by
\cite{munz_traffic_2010} :

% Fix these equations = add initial value
\begin {equation*}
\centering
\begin {split}
&\hat{x}_t = L_{t-1} + T_{t-1}, \\
&L_t = \alpha x_t + (1-\alpha)(L_{t-1} + T_{t-1}), \\
&T_t = \beta (L_t - L_{t-1}) + (1-\beta)T_{t-1}.
\end {split}
\label{eq:double_exp_smooth}
\end {equation*}

\par And triple exponential smoothing is defined by:

\begin {equation*}
\centering
\begin {split}
&\hat{x}_t = L_{t-1} + T_{t-1} + I_{t-1},\\
&L_t = \alpha (x_t - I_{t-s}) + (1-\alpha)(L_{t-1} + T_{t-1}), \\
&T_t = \beta (L_t - L_{t-1}) + (1-\beta)T_{t-1}, \\
&I_t = \gamma (x_t - L_t) + (1-\gamma)I_{t-s}.
\end {split}
\label{eq:triple_exp_smooth}
\end {equation*}

\par The components these methods introduce account for the cyclic behaviours in the data: $L_t$ accounts for the baseline behaviour of the data, which is 
calculated on the simple method; $T_t$ smooths the trend with the $\beta$ parameter; and $S_t$ accounts for the seasonal components with the $\gamma$ parameter.
As with $\alpha$ these parameters should be defined mostly by previous experience.

\subsubsection{Autoregressive Moving average}

\par Approximating the time series data to a model allows for generating predictions for the next values, since the relationship between two variables in the data is
then known. The Box-Jenkins method defines the steps of building this model as \cite{box_time_2016}:

\begin {enumerate}
  \item \textbf{Identification} Model the data, by reducing the variables to a stationary state, and removing the possible seasonality in the series
  \item \textbf{Fitting} Estimate the parameters for the model
  \item \textbf{Checking} Verify if the model accurately fits the available data, returning to the identification step if its not adequate
\end {enumerate}

\par Modelling the time series data is possible through moving average (MA), autoregression (AR) or a mix of the two (ARMA) processes. 
The autoregressive model, also referred to as AR(\textit{p}), considers the linear regression of the $p$ past values, as equation \ref{eq:ar} shows.

\begin{equation}
    x_t = \sum^p_{i=1}\phi_i X_{t-i} + \epsilon_t,
    \label{eq:ar}
\end{equation}

\par where $\epsilon_t$ is the white noise component, a purely random process of mean 0 and variance $\sigma^2$, $\phi_i$ are the constant parameters of the model,
and $p$ defines the order of the model.

\par The moving average process models the time series to the white noise that has occurred in the previous periods, as shown in equation \ref{eq:ma}. In this
equation, $\theta_i$ are the parameters of the model, $\mu$ is the mean of the series, and $\epsilon_t$ are the white noise components. Similarly as in the 
autoregressive model, the moving average process can be defined as MA(q), where $q$ is the order of this model.

\begin{equation}
    x_t = \sum^q_{i=1}\theta_i\epsilon_i + \mu,
    \label{eq:ma}
\end{equation}

\par Finally, the ARMA($p$, $q$) process is defined as the combination of both methods, as shown in equation \ref{eq:arma}.

\begin{equation}
    x_t = \mu + \sum^q_{i=1}\theta_i\epsilon_i + \sum^p_{i=1}\phi_i X_{t-i}.
    \label{eq:arma}
\end{equation}

\par Finding the $p$ and $q$ parameters of the models is done via observation of autocorrelation and partial autocorrelation functions, as introduced in the
Box-Jenkins method \cite{box_time_2016}. The autocorrelation function calculates the correlation of a time series with its own lagged values. The behaviour 
of this function can provide us with the information on the these parameters: if the sample autocorrelation shows an exponential decrease then the process can be 
modelled with an AR method; and if this function shows a drop after a certain value $q$, then the moving average model is better suited for modelling the time series
\cite{munz_traffic_2010}.

\subsubsection{Error calculation}

A very important part of building forecasting modules is assessing the associated errors with the prediction. Minimizing prediction errors improves the quality of the
forecasts, via the adjustment of the chosen parameters for the model.

\par A common way to calculate the error is via the Squared Sum of Errors (SSE), which is shown in equation \ref{eq:sse}. 

\begin {equation} 
    \label{eq:sse}
    SSE = \sum_{t=1}^T{(x_t-\hat{x}_{t})^2} = \sum_{t=1}^T{\epsilon_t^2}
\end {equation}

\par Other methods include the Root Mean Squared Error (RMSE),

\begin {equation*} 
    \label{eq:mape}
     RMSE = \sqrt{\sum_{t=1}^T\frac{(x_t-\hat{x}_{t})^2}{n}} 
\end {equation*}

\par or the Mean Absolute Percentage Error (MAPE),

\begin {equation*} 
    \label{eq:mape}
    MAPE = \frac{100}{n} \sum_{t=1}^T{\left | \frac{x_t - \hat{x}_t}{x_t} \right |},
\end {equation*}

In \cite{hyndman_another_2006} a comparative study of these measures is presented to determine the most adequate measure for univariate time series forecasts. They
analyse simple forecasting methods like the exponential smoothing and Holt's method, and the out-of-sample \footnote{used for evaluating the forecasting 
accuracy, by using samples not belonging to the initial fitting period} and in-sample \footnote{using data available on the initial fitting period to generate one 
step ahead forecasts} performance of these methods. This analysis of the different forecasting accuracy measures is finished by concluding that the Mean Absolute
Scaled Error (MASE), seen in equation \ref{eq:mase} due to simple interpretations, and independent of the scale of the data, which poses problems when comparing 
forecasts with different scales. In equation \ref{eq:mase}, the numerator $e_t$ is the forecast error for a certain period, or $e_t = x_t - \hat{x}_t$.

\begin {equation} 
    \label{eq:mase}
    MASE = \frac{1}{T} \sum_{t=1}^T{\left ( \frac{\left | e_t \right |}{\frac{1}{T-1}\sum_{t=2}^T \left | x_t - x_{t-1} \right |} \right )}
\end {equation}

\subsubsection {Change Detection} \label{sec:change_detection}

A relevant indicator for the validity of the generated forecasts is the forecast error, that is calculated via the difference between the real measurements and the
predicted value. Furthermore, by assuming a distribution for the forecast error, and a certain significance level, it is possible to validate the generated forecasts,
and detect values that do not fit the model, accusing a possible variation in the parameters of the model. As such, the prediction error is able to be employed in 
change detection algorithms. This prediction error can be defined as:

\begin {equation*}
\centering
\epsilon_t = x_t - \hat{x}_t
\end {equation*}

\par \textbf{Hypothesis testing} is used to perform a test of an assumption about two random variables. This hypothesis states that a certain relationship between
the two variables exists with a certain significance value, and this relationship can be a relation in the means, the distribution of the observations, etc. 
The first step is set \textbf{null hypothesis}, which is the desired assumption to test, and is referred to as $H_0$, which is then tested
against the \textbf{alternative hypothesis}, $H_1$, which is considered true if $H_0$ is rejected. The validity of $H_0$ is based on a comparison between the two
data sets, according to a certain threshold, called the significance level. This significance level also defines the probability of wrongly rejecting or accepting a
hypothesis, which are defined as following errors: \textbf{Type I error}, happening when the null hypothesis is rejected, but the performed test is true, and the 
\textbf{Type II errors}, occurring when the opposite happens.

\par Originating from quality engineering, \textbf{control charts} are common ways of following the output of a certain process, with the aim of reducing variability
associated to manufacturing processes. Control charts allow to evaluate the possible sources of variation, and classify the output of the process, based on the mean
or variation of the sampled process, as \textbf{in control} or \textbf{out of control}, depending on the causes of variation. The process is considered in control
when the parameters of the monitored variable, like $\mu_0$ or $\sigma^2$ are inside the predefined \textbf{control limits}, that are usually set as requirements for
the process. The wide range of control charts allow for a flexibility in choosing the right one that fits each application. Common charts used are those proposed by 
Shewhart, where the measurements from the process in study provide a statistic, like the mean, range, etc. Plotting these parameters allows for drawing the center
line at $H_0$, and the control limits are defined by a multiple of the standard deviation. This control chart allows for actions to the process be performed not only
when the points are shown to be out of control, but also when there is a sequence of values above or below the center line, or a upward or downward trend is shown in
the control charts.In the context of change detection, the hypothesis test relies on $H_0$ stating that there is no change in the parameters of the sample like the
mean or the variation, and the second hypothesis $H_1$ stating the contrary.

\par The CUSUM (\textbf{cu}mulative \textbf{su}m) control chart provides a test based on \textit{stopping rules}, where the alarms are raised when a parameter
of the distribution $\theta_t$, like the mean, or the variance exceeds certain thresholds. In the parametric case, the CUSUM algorithm for detection of a change at
$t_0$ from the observation $x_i$ is based on the log likelihood ratio defined by \cite{ahmed_novel_2008}.

\begin{equation*}
\centering
    S_t = \sum_{i=1}^k s_i = \sum_{i=1}^k\ln \frac {P_{\theta_1}(x_i)} {P_{\theta_0}(x_i)}.
\end{equation*}

\par The previous equation is related to the negative drift of $S_t$ under normal conditions, and the positive drift after a change is detected. The alarm is raised 
when a test statistic $g_t$ is larger than a threshold $h$, and can be obtained by 

\begin{equation*}
\centering
    \begin{split}
        &g_t = S_t - \min_{1 \leq i \leq t} Si \geq h
    \end{split}
\end{equation*}

\par In the case when $P_{\theta}(x)$ is not known, the log likelihood parameter cannot be calculated, a non-parametric approach must be used, as mentioned in \cite{
ahmed_novel_2008}.  

\subsubsection {Performance Evaluation} \label{sec:performance_evaluation}

A relevant metric for designing change detection models for application under network traffic is the \textit{false alarm rate}, due to the impact that incorrectly 
identifying the normal state of the network as abnormal can have on the operators capability of addressing real abnormalities. This statistical difference of errors
present in hypothesis testing is not relevant for network operators \cite{munz_traffic_2010}, which implies that the design of the change detection mechanisms should
reduce the total false alarm rate. 
The effectiveness of a statistical approach for change detection can be seen as the relationship between false and real alarms, since the trade-off with online
detection is the number of falsely raised alarms, and the number of accurately reported changes. As mentioned in section \ref{sec:change_detection} the possible 
wrong decisions are the type I and II errors, which are usually represented with error, or confusion matrices, seen in table 3.2, 
where the possible outcomes from the decision algorithm are displayed. The notation for this table is True Positive (T\textsubscript{P}), False Positive 
(F\textsubscript{P}), False Negative (F\textsubscript{N}) and True Negative (T\textsubscript{N}).

\begin{table}[h]
\centering
\begin{tabular}{ccc|c}
                                                          &&  \multicolumn{2}{c}{Predicted}  \\ 
                                                          && $H_0$  &  $H_1$                                    \\   \cline{2-4}
        \multirow{2}{*}{\rotatebox[origin=c]{90}{Actual}} & \multicolumn{1}{l|}{$H_0$}    & T\textsubscript{P}     & F\textsubscript{P}       \\   \cline{2-4}
                                                          & \multicolumn{1}{l|}{$H_1$}     & F\textsubscript{N}     & T\textsubscript{N}     \\   
\end{tabular}
\caption{Generalized confusion matrix for hypothesis testing}
\end{table}

\par In the following equations, we define the ratios that allow for measuring the performance of the algorithm, with Accuracy (A), measuring the correct decision
of the algorithm; Sensitivity (S), indicating the capability of the system to detect the change, and Precision (P) indicates the ability of the algorithm to
accurately distinguish between true and false alarms. 

\begin{equation*}
\begin{split}
    &A    =  \frac {T_P + T_N} {T_P + T_N + F_P + F_N} = \frac {T_P + T_N} {N_{alarms}}   \\
    &S    =  \frac {T_P} {T_P + F_N} \\
    &P    =  \frac {T_P} {T_P + F_P}
\end{split}
\end{equation*}

The performance of the change detection algorithms can also be defined by:

\begin{itemize}
    \item MTFA (Mean Time Between False Alarms) 
    \item MTD (Mean Time to Detection)
    \item ARL (Average Run Length)
\end{itemize}

% XXX - Quote this

\par The ARL is the expected number of samples required before an alarm is provided, and it can be divided further in two important measures: ARL\textsubscript{0},
which specifies the expected number of required samples until the alarm is raised, assuming that the process is in control; and the ARL\textsubscript{1}, indicating
the expected number of samples until an alarm is raised, under the condition that the process is out of control. For optimal results for change detection, we require
that ARL\textsubscript{0} is as large as possible, and ARL\textsubscript{1} be as small as possible.
