\chapter{Statistical Detection} \label{chap:stat_det} %% chapter 3

As a result of the large scale of current data centers, maintaining control over these networks proves a difficult task. Networks operators must then adapt to the current situation by improving the monitoring infrastructures 
to allow faster response to problems.  Building a feature complete management API for a SDN controller means that information obtained from the port and flow statistics previously implemented should return information 
about the global state of the network, so that root-cause analysis of the source of network issues can be done faster and easier, which reflect on better service and lower costs for network operators. 
\par Network behaviour analysis is defined by the constant monitoring of a network, so that events that compromise the "healthy" state of the network can be removed or mitigated. These include not only cases where the anomalies are
caused with malicious intent, like the case of DDoS attacks, but also failure of network devices or changes in user behaviour \cite {traffic_anomaly_control_charts}. These systems are equipped with alarm capabilities, so that 
system administrators can quickly respond to changes, possibly even giving some information about the source of the problem. However, the automation of these monitoring processes means that the possible existence
of false alarms reduces the operators capabilities to act on actual failures. 
\par This chapter focuses on the recent research done in order to implement systems that rely on statistical analysis for monitoring the state of the networks and detection of abnormalities.

\section {Elephant flow detection}

Detection of network anomalies is subject to intense research, and as such, several methods were developed, that assume different levels of control over the network and provide different results to different applications. Our goal
in this section is then to provide a description of the different types of network issues that can occur, and some proposed solutions for these issues.
\par The problem of detection of traffic anomalies has been subject to extensive research, and several different approaches have been proposed. These methods are based on different techniques 
\cite {http://shiftleft.com/mirrors/www.hpl.hp.com/personal/Praveen_Yalagandula/papers/INFOCOM11.pdf}:

\begin {itemize}
    \item Modifications to the applications and services to notify the controller about the state of the traffic on each service. Despite this approach resulting in the most accurate "detection" of network anomalies, the support 
for this technique is not extensive, due to the required changes to each service, and it does not account for abrupt changes on the traffic
    \item By setting hard limits on the transmission capabilities of each port and switch, the controller is ensured of the non existence of flows that could impact network performance. This is a mitigation strategy that
does not scale well to very large networks, since it requires the storage of the rules imposed to every port, and can potentially lead to the inefficient use of network resources, reducing flexibly on the DCN's.
    \item By employing sampling and collection techniques, using mechanisms like sFlow \ref{subsec:sflow}, and building the profiles of the normal state of the network, this method can detect outliers that deviate from the
normal state of the network. Utilizing this method reduces impact that continuously polling the network might have, while reducing impact on the packet and byte counts 
\cite {https://www.cert.org/flocon/2006/presentations/packet_sample_anomoly2006.pdf}, but the loss of information inherent to sampling may be a challenge on successful deployment, which should account for optimal sampling 
strategies and inference from the obtained statistics.
    \item Periodic polling of the statistics from the switch, and employing statistical analysis methods to determine change points in the state of the network.
\end {itemize}

\section {Time series analysis}

Understanding processes and their results is a key factor in the success of implementing new features, or analysing existing ones for their efficiency and output. This analysis, important for the different fields in engineering,
economics, health, allows obtaining information about the normal and abnormal state of each underlying process, provide forecasts and predictions on short and long term behaviour of the relevant data, classify and cluster 
information, and more. The act of collecting and processing the data, over a period of time is called \textit{time series analysis}. 
\par Monitoring systems provide a guarantee in quality engineering, since they allow to follow a system and its properties, and notify operators if changes happen that impact the normal status of a relevant parameter. These changes
can be occasional or systematic, but should the state of the system deviate from the limits set by the operators, researching the cause of the errors can reveal some errors or malfunctions in the system. \textit{Change detection}
is the study of the different parameters of the system, and determining the points where these cause a significant deviation from the normal operation. In network traffic analysis, these methods can be applied to determine when 
the behaviour of certain flows, that can be monitored over metrics originating from the controller and switches, impact the traffic characteristics. One key part on the application of changepoint detection is the understanding 
and selection of the proper metrics to monitor, to ensure that these are sensitive to the traffic changes. One other important consideration in applying changepoint detection mechanisms is the reduction of false alarms, that occur 
when the metrics are too sensitive to traffic changes, and limit the network operators capability of accurately responding to real network issues. 
\par Change detection mechanisms are classified as follow \cite { CITE -https://www.net.in.tum.de/fileadmin/TUM/NET/NET-2010-06-1.pdf#cite.feinstein03}:

\begin {itemize}
    \item One of the most important distinctions in change detection theory is the difference between online and offline detection. Offline, or batch detection methods consider a fixed length of observations, and retrospectively 
analyse the dataset to determine the time where the change, or changes took place. Online detection, or sequential detection, unlike batch detection which uses all the available observations to detect the changes, including the
ones obtained after the change took place, is based on the determination of the change points based on the arrival of the new data, allowing for determining the change as fast as possible.
\cite { CITE - http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.425.1477&rep=rep1&type=pdf}. 
    % TODO - Bayesian change detection
    \item An important criteria for the classification of changepoint detection algorithms is the knowledge of the distribution that the system has before the change occurs. A Bayesian system considers the 
    \item Finally, the last important distinction is related to the scalability, and relate to the amount of data that needs to be stored to accurately implement detection on new samples. Parametric approaches rely on learning a 
probability distribution from the monitored variables, and using this learned data to estimate the unknown parameters, after which the training data can be discarded. Non-parametric models however, do not take into consideration the
distribution of the monitored variables, and analyse statistical properties instead. The cost of this analysis is that the previous data must be stored to provide better results, but this problem can be mitigated using 
algorithms using sliding window or moving averages.
\end {itemize}

\subsection {Forecasting models}

\subsection {Mathematical formulations}

A time series can be defined as a stream of observations $X = \{x_1, ... x_i\}$, where $x_i$ is a vector arriving at time $i$. The time series $X$ can also be described as the sum of the following components: $S_t$,
which refers to the seasonal component of the data; $T_t$, which defines the trend of the data, and $\epsilon_t$ represents the residual values, accounting for non expected variation and noise.

\begin {equation*}
\centering
y_t = S_t + T_t + \epsilon_t
\end {equation*}

Or:

\begin {equation*}
\centering
y_t = S_t * T_t * \epsilon_t
\end {equation*}

\par In regards to the classification of the trends according to the type of change, they can be classified as:

\begin {itemize}
  \item \textbf {Deterministic} when the trend consistently increase/decrease
  \item \textbf {Stochastic} when the opposite happens
\end {itemize}

\par Approximating the trend to a certain model allows for generating predictions for the next values, since the relationship between two variables in the data is then known. The most common models present in trends are:

\begin{table}[h]
\centering
\label{tab:trends}
\begin{tabular}{|c|c|}
\hline
Linear      & $y = m \cdot x + b$ \\ \hline
Polynomial  & $y = b + c_1 \cdot x + \dotso + c_n \cdot x^n$  \\ \hline
Exponential & $y = c \cdot  x^b$     \\ \hline
Logarithmic & $y = a \cdot ln(x) + b$        \\ \hline
\end{tabular}
\caption{Trend models}
\end{table}

\par Time series data usually present a non stationary behaviour, that are characterized by changes in the mean and variance. Statistical methods require, however, stationarity in the data. The presence of trends and cyclic behaviours
is the most common violation of stationary, and can be removed through differencing.

\begin {equation*}
\centering
\nabla X_t = X_t - X_{t-1} = (1-B)X_t
\end {equation*}

Where,

\begin {equation*}
\centering
B X_t = X_{t-1}
\end {equation*}

\par Under the assumption that the underlying process can be modeled by previous historical values, and assuming this model remains true for future measurements, this can be used to generate forecasts for future values in the time series. A relevant indicator for the
validity of the generated forecasts is the forecast error, that is calculated via the difference between the real measurements and the predicted value. Furthermore, by assuming a distribution for the forecast error, and a certain significance level, it is possible 
to validate the generated forecasts, and detect values that do not fit the model, accusing a possible variation in the parameters of the model. As such, the prediction error is able to be employed in change detection algorithms. This
prediction error can be defined as:

\begin {equation*}
\centering

\end {equation*}
 
\subsection {Change Detection}
\subsection {Control charts}
