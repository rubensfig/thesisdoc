\section{Statistical Detection} \label{chap:stat_det}

As a result of the large scale of current data centers, maintaining control over these networks proves a difficult task. Networks operators must then adapt to the current situation by improving the monitoring infrastructures 
to allow faster response to problems.  Building a feature complete management API for a SDN controller means that information obtained from the port and flow statistics previously implemented should return information 
about the global state of the network, so that root-cause analysis of the source of network issues can be done faster and easier, which reflect on better service and lower costs for network operators. 
\par Network behaviour analysis is defined by the constant monitoring of a network, so that events that compromise the "healthy" state of the network can be removed or mitigated. These include not only cases where the anomalies are
caused with malicious intent, like the case of DDoS attacks, but also failure of network devices or changes in user behaviour \cite{munz_traffic_2010}. These systems are equipped with alarm capabilities, so that 
system administrators can quickly respond to changes, possibly even giving some information about the source of the problem. However, the automation of these monitoring processes means that the possible existence
of false alarms reduces the operators capabilities to act on actual failures. 
\par This chapter focuses on the recent research done in order to implement systems that rely on statistical analysis for monitoring the state of the networks and detection of abnormalities.

\subsection {Time series analysis}

Understanding processes and their results is a key factor in the success of implementing new features, or analysing existing ones for their efficiency and output. This analysis, important for the different fields in engineering,
economics, health, allows obtaining information about the normal and abnormal state of each underlying process, provide forecasts and predictions on short and long term behaviour of the relevant data, classify and cluster 
information, and more. The act of collecting and processing the data, over a period of time is called \textit{time series analysis}. 
\par Monitoring systems provide a guarantee in quality engineering, since they allow to follow a system and its properties, and notify operators if changes happen that impact the normal status of a relevant parameter. These changes
can be occasional or systematic, but should the state of the system deviate from the limits set by the operators, researching the cause of the errors can reveal some errors or malfunctions in the system. \textit{Change detection}
is the study of the different parameters of the system, and determining the points where these cause a significant deviation from the normal operation. In network traffic analysis, these methods can be applied to determine when 
the behaviour of certain flows, that can be monitored over metrics originating from the controller and switches, impact the traffic characteristics. One key part on the application of changepoint detection is the understanding 
and selection of the proper metrics to monitor, to ensure that these are sensitive to the traffic changes. One other important consideration in applying changepoint detection mechanisms is the reduction of false alarms, that occur 
when the metrics are too sensitive to traffic changes, and limit the network operators capability of accurately responding to real network issues. 
\par Change detection mechanisms are classified as follow  \cite{munz_traffic_2010}:

\begin {itemize}
    \item One of the most important distinctions in change detection theory is the difference between online and offline detection. Offline, or batch detection methods consider a fixed length of observations, and retrospectively 
analyse the dataset to determine the time where the change, or changes took place. Online detection, or sequential detection, unlike batch detection which uses all the available observations to detect the changes, including the
ones obtained after the change took place, is based on the determination of the change points based on the arrival of the new data, allowing for determining the change as fast as possible \cite{ahmed_novel_2008}. 
    % TODO - Bayesian change detection, read Bayes filter
    \item An important criteria for the classification of changepoint detection algorithms is the knowledge of the distribution that the system has before the change occurs. A Bayesian system considers the 
    \item Finally, the last important distinction is related to the scalability, and relate to the amount of data that needs to be stored to accurately implement detection on new samples. Parametric approaches rely on learning a 
probability distribution from the monitored variables, and using this learned data to estimate the unknown parameters, after which the training data can be discarded. Non-parametric models however, do not take into consideration the
distribution of the monitored variables, and analyse statistical properties instead. The cost of this analysis is that the previous data must be stored to provide better results, but this problem can be mitigated using 
algorithms using sliding window or moving averages.
\end {itemize}

\subsubsection {Mathematical formulations}

A time series can be defined as a stream of observations $X = \{x_1, ... x_i\}$, where $x_i$ is a vector arriving at time $i$. The time series $X$ can also be described as the sum of the following components: $S_t$,
which refers to the seasonal component of the data; $T_t$, which defines the trend of the data, and $R_t$ represents the residual values, accounting for non expected variation and noise.

\begin {equation*}
\centering
y_t = S_t + T_t + R_t
\end {equation*}

Or:

\begin {equation*}
\centering
y_t = S_t * T_t * R_t
\end {equation*}

\par In regards to the classification of the trends according to the type of change, they can be classified as:

\begin {itemize}
  \item \textbf {Deterministic} when the trend consistently increase/decrease
  \item \textbf {Stochastic} when the opposite happens
\end {itemize}

\par Approximating the time series data to a model allows for generating predictions for the next values, since the relationship between two variables in the data is
then known. The Box-Jenkins method defines the steps of building the model as \cite{box_time_2016}:

\begin {enumerate}
  \item \textbf{Identification} Model the data, by reducing the variables to a stationary state, and removing the possible seasonality in the series
  \item \textbf{Fitting} Estimate the parameters for the model
  \item \textbf{Checking} Verify if the model accurately fits the available data, returning to the identification step if its not adequate
\end {enumerate}

\begin{table}[h]
\centering
\begin{tabular}{c|c}
Linear      & $y = m \cdot x + b$ \\ \hline
Polynomial  & $y = b + c_1 \cdot x + \dotso + c_n \cdot x^n$  \\ \hline
Exponential & $y = c \cdot  x^b$     \\ \hline
Logarithmic & $y = a \cdot ln(x) + b$
\end{tabular}
\caption{Trend models}
\label{tab:trends_here}
\end{table}

\par Time series data usually present a non stationary behaviour, that are characterized by changes in the mean and variance. Statistical methods require, however,
stationarity in the data. The presence of trends and cyclic behaviours is the most common violation of stationary, and table \ref{tab:trends_here} shows the most 
common trends present in the data, and the parameters that need to be learned from the time series in study. Removing systematic changes like trends and seasonal 
variation is possible with differencing:

\begin {equation*}
\centering
\nabla X_t = X_t - X_{t-1}
\end {equation*}

\par By modelling the time series data with moving average (MA), autoregression (AR) or a mix of the two (ARMA) processes, under the assumption that the underlying
process can be modeled by previous historical values, and assuming this model remains true for future measurements, the time series data historical behaviour can
be used to generate forecasts for future values in the time series. The one step ahead prediction is referred as $\hat{x}_t$.

\par Exponential smoothing allows for generating predictions using the historical behaviour, but differs from normal MA methods by applying a set of weights to the
data that exponentially decreases over time. Considering the time series $X_t$, the prediction $\hat{x}_t$ can be obtained by the equation \ref{eq:exp_smooth}. 
In this model, the smoothing factor $\alpha (0 < \alpha < 1)$ is obtained empirically, and its value will determine the forgetting factor for the past observations.

\begin {equation*}
\centering
\begin {split}
&\hat{x}_1 = x_0 \\
&\hat{x}_t = \alpha x_{t-1} + (1-\alpha)x_0, t > 1
\end {split}
\label{eq:exp_smooth}
\end {equation*}

\par Due to its simplicity, this method is not suitable for situations where the data has trends, or seasonal behaviours \cite{kalekar_time_2004}. The solution for
this problem is introduced with double exponential smoothing, also know as Holt forecasting, and triple exponential smoothing, also known as Holt-Winters
forecasting. These methods introduce further components to dampen the effects of cyclic behaviours in the data. Double exponential smoothing is defined by
\cite{munz_traffic_2010} :

% Fix these equations = add initial value
\begin {equation*}
\centering
\begin {split}
&\hat{x}_t = L_{t-1} + T_{t-1} \\
&L_t = \alpha x_t + (1-\alpha)(L_{t-1} + T_{t-1}) \\
&T_t = \beta (L_t - L_{t-1}) + (1-\beta)T_{t-1}
\end {split}
\label{eq:double_exp_smooth}
\end {equation*}

\par And triple exponential smoothing is defined by:

\begin {equation*}
\centering
\begin {split}
&\hat{x}_t = L_{t-1} + T_{t-1} + I_{t-1}\\
&L_t = \alpha (x_t - I_{t-s}) + (1-\alpha)(L_{t-1} + T_{t-1}) \\
&T_t = \beta (L_t - L_{t-1}) + (1-\beta)T_{t-1} \\
&I_t = \gamma (x_t - L_t) + (1-\gamma)I_{t-s}
\end {split}
\label{eq:triple_exp_smooth}
\end {equation*}

\par The components these methods introduce account for the cyclic behaviours in the data: $L_t$ accounts for the baseline behaviour of the data, which is 
calculated on the simple method; $T_t$ smooths the trend with the $\beta$ parameter; and $S_t$ accounts for the seasonal components with the $\gamma$ parameter.
As with $\alpha$ these parameters are defined mostly by previous experience, but a common optimization is the minimization of the square sum of prediction errors, defined by:

\begin {equation*} 
    \label{eq:sse}
    SSE = \sum_{t=1}^T{(x_t-\hat{x}_{t})^2} = \sum_{t=1}^T{\epsilon_t^2}
\end {equation*}

\subsubsection {Change Detection} \label{sec:change_detection}

A relevant indicator for the validity of the generated forecasts is the forecast error, that is calculated via the difference between the real measurements and the
predicted value. Furthermore, by assuming a distribution for the forecast error, and a certain significance level, it is possible to validate the generated forecasts,
and detect values that do not fit the model, accusing a possible variation in the parameters of the model. As such, the prediction error is able to be employed in 
change detection algorithms. This prediction error can be defined as:

\begin {equation*}
\centering
\epsilon_t = x_t - \hat{x}_t
\end {equation*}

\par \textbf{Hypothesis testing} is used to perform a test of an assumption about two random variables. This hypothesis states that a certain relationship between
the two variables exists with a certain significance value, and this relationship can be a relation in the means, the distribution of the observations, etc. 
The first step is set \textbf{null hypothesis}, which is the desired assumption to test, and is referred to as $H_0$, which is then tested
against the \textbf{alternative hypothesis}, $H_1$, which is considered true if $H_0$ is rejected. The validity of $H_0$ is based on a comparison between the two
data sets, according to a certain threshold, called the significance level. This significance level also defines the probability of wrongly rejecting or accepting a
hypothesis, which are defined as following errors: \textbf{Type I error}, happening when the null hypothesis is rejected, but the performed test is true, and the 
\textbf{Type II errors}, occurring when the opposite happens.

\par Originating from quality engineering, \textbf{control charts} are common ways of following the output of a certain process, with the aim of reducing variability
associated to manufacturing processes. Control charts allow to evaluate the possible sources of variation, and classify the output of the process, based on the mean
or variation of the sampled process, as \textbf{in control} or \textbf{out of control}, depending on the causes of variation. The process is considered in control
when the parameters of the monitored variable, like $\mu_0$ or $\sigma^2$ are inside the predefined \textbf{control limits}, that are usually set as requirements for
the process. The wide range of control charts allow for a flexibility in choosing the right one that fits each application. Common charts used are those proposed by 
Shewhart, where the measurements from the process in study provide a statistic, like the mean, range, etc. Plotting these parameters allows for drawing the center
line at $H_0$, and the control limits are defined by a multiple of the standard deviation. This control chart allows for actions to the process be performed not only
when the points are shown to be out of control, but also when there is a sequence of values above or below the center line, or a upward or downward trend is shown in
the control charts.

\par In the context of change detection, the hypothesis test relies on $H_0$ stating that there is no change in the parameters of the sample, like the mean or the
variation, and the second hypothesis $H_1$ stating the contrary. A relevant metric for designing change detection models for application under network traffic is the
\textit{false alarm rate}, due to the impact that incorrectly identifying the normal state of the network as abnormal can have on the operators capability of 
addressing real abnormalities. The previously presented statistical difference of errors present in hypothesis testing are not relevant from the network operators
point of view \cite{munz_traffic_2010},
which implies that the design of the change detection mechanisms should reduce the false alarm rate. In section \ref{subsec:performance_evaluation} some mechanisms
for these optimizations are explored.

\par The CUSUM (\textbf{cu}mulative \textbf{su}m) control chart provides a test based on \textit{stopping rules}, where the alarms are raised when the parameter
of the distribution, $\theta_t$ exceeds certain thresholds. In the parametric case, the CUSUM algorithm for detection of a change at $t_0$ from the observation
$x_i$ is based on the log likelihood ratio defined by \cite{ahmed_novel_2008}.

\begin{equation*}
\centering
    S_t = \sum_{i=1}^k s_i = \sum_{i=1}^k\ln \frac {P_{\theta_1}(x_i)} {P_{\theta_0}(x_i)}.
\end{equation*}

\par The previous equation is related to the negative drift of $S_t$ under normal conditions, and the positive drift after a change is detected. The alarm is raised 
when a test statistic $g_t$ is larger than a threshold $h$, and can be obtained by 

\begin{equation*}
\centering
    \begin{split}
        &g_t = S_t - \min_{1 \leq i \leq t} Si \geq h
    \end{split}
\end{equation*}

\par In the case when $P_{\theta}(x)$ is not known, the log likelihood parameter cannot be calculated, a non-parametric approach must be used, as mentioned in \cite{
ahmed_novel_2008}.  

\subsubsection {Performance Evaluation} \label{subsec:performance_evaluation}

The effectiveness of a statistical approach for change detection can be seen as the relationship between false and real alarms, since the trade-off with online
detection is the number of falsely raised alarms, and the number of accurately reported changes. As mentioned in section \ref{sec:change_detection} the possible 
wrong decisions are the type I and II errors, which are usually represented with error, or confusion matrices, seen in table 3.2, 
where the possible outcomes from the decision algorithm are displayed. The notation for this table indicate the possible values, True Positive (T\textsubscript{P}),
False Positive (F\textsubscript{P}), False Negative (F\textsubscript{N}) and True Negative (T\textsubscript{N}).

\begin{table}[h]
\centering
\begin{tabular}{ccc|c}
                                                          &&  \multicolumn{2}{c}{Predicted}  \\ 
                                                          && $H_0$  &  $H_1$                                    \\   \cline{2-4}
        \multirow{2}{*}{\rotatebox[origin=c]{90}{Actual}} & \multicolumn{1}{l|}{$H_0$}    & T\textsubscript{P}     & F\textsubscript{P}       \\   \cline{2-4}
                                                          & \multicolumn{1}{l|}{$H_1$}     & F\textsubscript{N}     & T\textsubscript{N}     \\   
\end{tabular}
\caption{Generalized confusion matrix for hypothesis testing}
\end{table}

\par In the following equations, we define the ratios that allow for measuring the performance of the algorithm, with Accuracy (A), measuring the correct decision
of the algorithm; Sensitivity (S), indicating the capability of the system to detect the change, and Precision (P) indicates the ability of the algorithm to
accurately distinguish between true and false alarms. 

\begin{equation*}
\begin{split}
    &A    =  \frac {T_P + T_N} {T_P + T_N + F_P + F_N} = \frac {T_P + T_N} {N_{alarms}}   \\
    &S    =  \frac {T_P} {T_P + F_N} \\
    &P    =  \frac {T_P} {T_P + F_P}
\end{split}
\end{equation*}

The performance of the change detection algorithms can also be defined by:

\begin{itemize}
    \item MTFA (Mean Time Between False Alarms) 
    \item MTD (Mean Time to Detection)
    \item ARL (Average Run Length)
\end{itemize}

% XXX - Quote this

\par The ARL is the expected number of samples required before an alarm is provided, and it can be divided further in two important measures: ARL\textsubscript{0},
which specifies the expected number of required samples until the alarm is raised, assuming that the process is in control; and the ARL\textsubscript{1}, indicating
the expected number of samples until an alarm is raised, under the condition that the process is out of control. For optimal results for change detection, we require
that ARL\textsubscript{0} is as large as possible, and ARL\textsubscript{1} be as small as possible.
