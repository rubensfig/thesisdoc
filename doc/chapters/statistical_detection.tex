\chapter{Statistical Detection} \label{chap:stat_det} %% chapter 3

As a result of the large scale of current data centers, maintaining control over these networks proves a difficult task. Networks operators must then adapt to the current situation by improving the monitoring infrastructures 
to allow faster response to problems.  Building a feature complete management API for a SDN controller means that information obtained from the port and flow statistics previously implemented should return information 
about the global state of the network, so that root-cause analysis of the source of network issues can be done faster and easier, which reflect on better service and lower costs for network operators. 
\par Network behaviour analysis is defined by the constant monitoring of a network, so that events that compromise the "healthy" state of the network can be removed or mitigated. These include not only cases where the anomalies are
caused with malicious intent, like the case of DDoS attacks, but also failure of network devices or changes in user behaviour \cite {traffic_anomaly_control_charts}. These systems are equipped with alarm capabilities, so that 
system administrators can quickly respond to changes, possibly even giving some information about the source of the problem. However, the automation of these monitoring processes means that the possible existence
of false alarms reduces the operators capabilities to act on actual failures. 
\par This chapter focuses on the recent research done in order to implement systems that rely on statistical analysis for monitoring the state of the networks and detection of abnormalities.

\section {Elephant flow detection}

Detection of network anomalies is subject to intense research, and as such, several methods were developed, that assume different levels of control over the network and provide different results to different applications. Our goal
in this section is then to provide a description of the different types of network issues that can occur, and some proposed solutions for these issues.
\par The problem of detection of traffic anomalies has been subject to extensive research, and several different approaches have been proposed. These methods are based on different techniques 
\cite {http://shiftleft.com/mirrors/www.hpl.hp.com/personal/Praveen_Yalagandula/papers/INFOCOM11.pdf}:

\begin {itemize}
    \item Modifications to the applications and services to notify the controller about the state of the traffic on each service. Despite this approach resulting in the most accurate "detection" of network anomalies, the support 
for this technique is not extensive, due to the required changes to each service, and it does not account for abrupt changes on the traffic
    \item By setting hard limits on the transmission capabilities of each port and switch, the controller is ensured of the non existence of flows that could impact network performance. This is a mitigation strategy that
does not scale well to very large networks, since it requires the storage of the rules imposed to every port, and can potentially lead to the inefficient use of network resources, reducing flexibly on the DCN's.
    \item By employing sampling and collection techniques, using mechanisms like sFlow \ref{subsec:sflow}, and building the profiles of the normal state of the network, this method can detect outliers that deviate from the
normal state of the network. Utilizing this method reduces impact that continuously polling the network might have, while reducing impact on the packet and byte counts 
\cite {https://www.cert.org/flocon/2006/presentations/packet_sample_anomoly2006.pdf}, but the loss of information inherent to sampling may be a challenge on successful deployment, which should account for optimal sampling 
strategies and inference from the obtained statistics.
    \item Periodic polling of the statistics from the switch, and employing statistical analysis methods to determine change points in the state of the network.
\end {itemize}

\section {Time series analysis}

Understanding processes and their results is a key factor in the success of implementing new features, or analysing existing ones for their efficiency and output. This analysis, important for the different fields in engineering,
economics, health, allows obtaining information about the normal and abnormal state of each underlying process, provide forecasts and predictions on short and long term behaviour of the relevant data, classify and cluster 
information, and more. The act of collecting and processing the data, over a period of time is called \textit{time series analysis}. 
\par Monitoring systems provide a guarantee in quality engineering, since they allow to follow a system and its properties, and notify operators if changes happen that impact the normal status of a relevant parameter. These changes
can be occasional or systematic, but should the state of the system deviate from the limits set by the operators, researching the cause of the errors can reveal some errors or malfunctions in the system. \textit{Change detection}
is the study of the different parameters of the system, and determining the points where these cause a significant deviation from the normal operation. In network traffic analysis, these methods can be applied to determine when 
the behaviour of certain flows, that can be monitored over metrics originating from the controller and switches, impact the traffic characteristics. One key part on the application of changepoint detection is the understanding 
and selection of the proper metrics to monitor, to ensure that these are sensitive to the traffic changes. One other important consideration in applying changepoint detection mechanisms is the reduction of false alarms, that occur 
when the metrics are too sensitive to traffic changes, and limit the network operators capability of accurately responding to real network issues. 
\par Change detection mechanisms are classified as follow \cite { CITE -https://www.net.in.tum.de/fileadmin/TUM/NET/NET-2010-06-1.pdf#cite.feinstein03}:

\begin {itemize}
    \item One of the most important distinctions in change detection theory is the difference between online and offline detection. Offline, or batch detection methods consider a fixed length of observations, and retrospectively 
analyse the dataset to determine the time where the change, or changes took place. Online detection, or sequential detection, unlike batch detection which uses all the available observations to detect the changes, including the
ones obtained after the change took place, is based on the determination of the change points based on the arrival of the new data, allowing for determining the change as fast as possible.
\cite { CITE - http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.425.1477&rep=rep1&type=pdf}. 
    % TODO - Bayesian change detection
    \item An important criteria for the classification of changepoint detection algorithms is the knowledge of the distribution that the system has before the change occurs. A Bayesian system considers the 
    \item Finally, the last important distinction is related to the scalability, and relate to the amount of data that needs to be stored to accurately implement detection on new samples. Parametric approaches rely on learning a 
probability distribution from the monitored variables, and using this learned data to estimate the unknown parameters, after which the training data can be discarded. Non-parametric models however, do not take into consideration the
distribution of the monitored variables, and analyse statistical properties instead. The cost of this analysis is that the previous data must be stored to provide better results, but this problem can be mitigated using 
algorithms using sliding window or moving averages.
\end {itemize}

\subsection {Mathematical formulations}

A time series can be defined as a stream of observations $X = \{x_1, ... x_i\}$, where $x_i$ is a vector arriving at time $i$. The time series $X$ can also be described as the sum of the following components: $S_t$,
which refers to the seasonal component of the data; $T_t$, which defines the trend of the data, and $R_t$ represents the residual values, accounting for non expected variation and noise.

\begin {equation*}
\centering
y_t = S_t + T_t + R_t
\end {equation*}

Or:

\begin {equation*}
\centering
y_t = S_t * T_t * R_t
\end {equation*}

\par In regards to the classification of the trends according to the type of change, they can be classified as:

\begin {itemize}
  \item \textbf {Deterministic} when the trend consistently increase/decrease
  \item \textbf {Stochastic} when the opposite happens
\end {itemize}

\par Approximating the time series data to a model allows for generating predictions for the next values, since the relationship between two variables in the data is then known. The Box-Jenkins method defines the steps of building the model as \cite{Box G.E.P. and G.E.P. Jenkins1976Time series analysis: Forecasting and control}:

\begin {enumerate}
  \item \textbf{Identification} Model the data, by reducing the variables to a stationary state, and removing the possible seasonality in the series
  \item \textbf{Fitting} Estimate the parameters for the model
  \item \textbf{Checking} Verify if the model accurately fits the available data, returning to the identification step if its not adequate
\end {enumerate}

\begin{table}[h]
\centering
\label{tab:trends}
\begin{tabular}{|c|c|}
\hline
Linear      & $y = m \cdot x + b$ \\ \hline
Polynomial  & $y = b + c_1 \cdot x + \dotso + c_n \cdot x^n$  \\ \hline
Exponential & $y = c \cdot  x^b$     \\ \hline
Logarithmic & $y = a \cdot ln(x) + b$        \\ \hline
\end{tabular}
\caption{Trend models}
\end{table}

\par Time series data usually present a non stationary behaviour, that are characterized by changes in the mean and variance. Statistical methods require, however, stationarity in the data. The presence of trends and cyclic behaviours
is the most common violation of stationary, and table \ref{tab:trends} shows the most common trends present in the data, and the parameters that need to be learned from the time series in study. Removing systematic changes like trends
and seasonal variation is possible with differencing:

\begin {equation*}
\centering
\nabla X_t = X_t - X_{t-1}
\end {equation*}

\par By modelling the time series data with moving average (MA), autoregression (AR) or a mix of the two (ARMA) processes, under the assumption that the underlying process can be modeled by previous historical values, and assuming
this model remains true for future measurements, the time series data historical behaviour can be used to generate forecasts for future values in the time series. The one step ahead prediction is referred as $\hat{x}_t$.

\par Exponential smoothing allows for generating predictions using the historical behaviour, but differs from normal MA methods by applying a set of weights to the data that exponentially decreases over time. Considering the time 
series $X_t$, the prediction $\hat{x}_t$ can be obtained by the equation \ref{eq:exp_smooth}. In this model, the smoothing factor $\alpha (0 < \alpha < 1)$ is obtained empirically, and its value will determine the forgetting factor
for the past observations.

\begin {equation*}
\centering
\begin {split}
&\hat{x}_1 = x_0 \\
&\hat{x}_t = \alpha x_{t-1} + (1-\alpha)x_0, t > 1
\end {split}
\label{eq:exp_smooth}
\end {equation*}

\par Due to its simplicity, this method is not suitable for situations where the data has trends, or seasonal behaviours \cite{CITE - http://labs.omniti.com/people/jesus/papers/holtwinters.pdf}. The solution for this problem is 
introduced with double exponential smoothing, also know as Holt forecasting, and triple exponential smoothing, also known as Holt-Winters forecasting. These methods introduce further components to dampen the effects of cyclic 
behaviours in the data. Double exponential smoothing is defined by \cite{CITE - https://www.net.in.tum.de/fileadmin/TUM/NET/NET-2010-06-1.pdf}:

\begin {equation*}
\centering
\begin {split}
&\hat{x}_t = L_{t-1} + T_{t-1} \\
&L_t = \alpha x_t + (1-\alpha)(L_{t-1} + T_{t-1}) \\
&T_t = \beta (L_t - L_{t-1}) + (1-\beta)T_{t-1}
\end {split}
\label{eq:exp_smooth}
\end {equation*}

\par And triple exponential smoothing is defined by:

\begin {equation*}
\centering
\begin {split}
&\hat{x}_t = L_{t-1} + T_{t-1} + I_{t-1}\\
&L_t = \alpha (x_t - I_{t-s}) + (1-\alpha)(L_{t-1} + T_{t-1}) \\
&T_t = \beta (L_t - L_{t-1}) + (1-\beta)T_{t-1} \\
&I_t = \gamma (x_t - L_t) + (1-\gamma)I_{t-s}
\end {split}
\label{eq:exp_smooth}
\end {equation*}

\par The components these methods introduce account for the cyclic behaviours in the data: $L_t$ accounts for the baseline behaviour of the data, which is calculated on the simple method; $T_t$ smooths the trend with the $\beta$ 
parameter; and $S_t$ accounts for the seasonal components with the $\gamma$ parameter. As with $\alpha$ these parameters are defined mostly by previous experience, but a common optimization is the minimization of the square sum 
of prediction errors, defined by:

\begin {equation*}
SSE = \sum_{t=1}^T{(x_t-\hat{x}_{t})^2} = \sum_{t=1}^T{\epsilon_t^2}
\end {equation*}

\subsection {Change Detection}

A relevant indicator for the validity of the generated forecasts is the forecast error, that is calculated via the difference between the real measurements and the predicted value. Furthermore, by assuming a distribution for 
the forecast error, and a certain significance level, it is possible to validate the generated forecasts, and detect values that do not fit the model, accusing a possible variation in the parameters of the model. As such,
the prediction error is able to be employed in change detection algorithms. This prediction error can be defined as:

\begin {equation*}
\centering
\epsilon_t = x_t - \hat{x}_t
\end {equation*}

\par \textbf{Hypothesis testing} is used to perform a test of an assumption about two random variables. This hypothesis states that a certain relationship between the two variables exists with a certain significance value, and this
relationship can be a relation in the means, the distribution of the observations, etc. The first step is set \textbf{null hypothesis}, which is the desired assumption to test, and is referred to as $H_0$, which is then tested
against the \textbf{alternative hypothesis}, $H_1$, which is considered true if $H_0$ is rejected. The validity of $H_0$ is based on a comparison between the two data sets, according to a certain threshold, called the significance
level. This significance level also defines the probability of wrongly rejecting or accepting a hypothesis, which are defined as following errors: \textbf{Type I error}, happening when the null hypothesis is rejected, but the 
performed test is true, and the \textbf{Type II errors}, occurring when the opposite happens.

\par Originating from quality engineering, \textbf{control charts} are common ways of following the output of a certain process, with the aim of reducing variability associated to manufacturing processes. Control charts allow to 
evaluate the possible sources of variation, and classify the output of the process, based on the mean or variation of the sampled process, as \textbf{in control} or \textbf{out of control}, depending on the causes of variation. 
The process is considered in control when the parameters of the monitored variable, like $\mu_0$ or $\sigma^2$ are inside the predefined \textbf{control limits}, that are usually set as requirements for the process.
The wide range of control charts allow for a flexibility in choosing the right one that fits each application. Common charts used are those proposed by Shewhart, where the measurements from the process in study provide a 
statistic, like the mean, range, etc. Plotting these parameters allows for drawing the center line at $H_0$, and the control limits are defined by a multiple of the standard deviation. This control chart allows for 
actions to the process be performed not only when the points are shown to be out of control, but also when there is a sequence of values above or below the center line, or a upward or downward trend is shown in the control charts.

\par In the context of change detection, the hypothesis test relies on $H_0$ stating that there is no change in the parameters of the sample, like the mean or the variation, and the second hypothesis $H_1$ stating the contrary.
A relevant metric for designing change detection models for application under network traffic is the \textit{false alarm rate},
due to the impact that incorrectly identifying the normal state of the network as abnormal can have on the operators capability of addressing real abnormalities. The previously presented statistical difference of errors 
present in hypothesis testing are not relevant from the network operators point of view \cite{CITE - https://www.net.in.tum.de/fileadmin/TUM/NET/NET-2010-06-1.pdf},
which implies that the design of the change detection mechanisms should reduce the false alarm rate. In section \ref{subsec:performance_evaluation} some mechanisms for these optimizations are explored.

\par The CUSUM (\textbf{cu}mulative \textbf{su}m) control chart provides a test based on \textit{stopping rules}, where the alarms are raised when the parameter of the distribution, $\theta_t$ exceeds certain thresholds. In
the parametric case, the CUSUM algorithm for detection of a change at $t_0$ from the observation $x_i$ is based on the log likelihood ratio defined by \cite {https://eprints.qut.edu.au/20572/3/ahmed2008npc.pdf}

\begin{equation*}
\centering
    S_t = \sum_{i=1}^k s_i = \sum_{i=1}^k\ln \frac {P_{\theta_1}(x_i)} {P_{\theta_0}(x_i)}.
\end{equation*}

\pagebreak

\par The previous equation is related to the negative drift of $S_t$ under normal conditions, and the positive drift after a change is detected. The alarm is raised when a test statistic $g_t$ is larger than a threshold $h$, and can be obtained by 

\begin{equation*}
\centering
    \begin{split}
        &g_t = S_t - \min_{1 \leq i \leq t} Si \geq h
    \end{split}
\end{equation*}

\par In the case when $P_{\theta}(x)$ is not known, the log likelihood parameter cannot be calculated, a non-parametric approach must be used, as mentioned in \cite{https://eprints.qut.edu.au/20572/3/ahmed2008npc.pdf}.  

% \section {Adaptive Thresholds}
\subsection {Performance Evaluation} \label{subsec:performance_evaluation}



\section {Existing methods}

